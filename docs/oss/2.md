name: Chapter-2
class: center,middle
.section[
Chapter 2
HashiCorp Consul Architecture
]

---
name: Introduction-to-Consul
Introduction to Consul
-------------------------
.center[![:scale 45%](images/consul_ref_arch.png)]

???
Consul is a complex system that has many different moving parts. To help users and developers of Consul form a mental model of how it works, this page documents the system architecture.

In the next sections, we will dive deeper into how  Consul works.

---
name: Introduction-to-Consul-Overview
Introduction to Consul - Overview
-------------------------

<br><br>
<img align="right" width="40%" src="images/consul_ref_arch.png">

* A consul cluster is refered to as a datacenter
* A consul datacenter is made up of server nodes and client nodes
* 3 or 5 server nodes per datacenter
* 100s - 10,000s of client nodes

???
Within each datacenter, we have a mixture of clients and servers. It is expected that there be between three to five servers. This strikes a balance between availability in the case of failure and performance, as consensus gets progressively slower as more machines are added. However, there is no limit to the number of clients, and they can easily scale into the thousands or tens of thousands.

---
name: Introduction-to-Consul-Gossip
Introduction to Consul - Gossip
-------------------------
<br><br>
<img align="right" width="40%" src="images/consul_ref_arch.png">

* All agent communication is done via the Gossip Protocol
* Automatic configuration and datacenter discovery for Consul agents
* Agent failures is done at the collective agent level not at the server level
* Using Gossip this allows high scalability vs. traditional heartbeat schemes
* Node failure can be inferred by an agent failure

???
All the agents that are in a datacenter participate in a gossip protocol. This means there is a gossip pool that contains all the agents for a given datacenter. This serves a few purposes: first, there is no need to configure clients with the addresses of servers; discovery is done automatically. Second, the work of detecting agent failures is not placed on the servers but is distributed. This makes failure detection much more scalable than naive heartbeating schemes. It also provides failure detection for the nodes; if the agent is not reachable, then the node may have experienced a failure.

---
name: Introduction-to-Consul-Consensus
Introduction to Consul - Consensus
-------------------------
<br><br>
<img align="right" width="40%" src="images/consul_ref_arch.png">

* Every Consul datacenter has a group of server nodes that work together to manage connected agents
* Using Raft the server nodes elect a leader
* A leader is responsible for processing all queries and has write authority to the KV store
* It is also responsible for transaction replication
* All requests to the server nodes are routed to the leader

???
The servers in each datacenter are all part of a single Raft peer set. This means that they work together to elect a single leader, a selected server which has extra duties. The leader is responsible for processing all queries and transactions. Transactions must also be replicated to all peers as part of the consensus protocol. Because of this requirement, when a non-leader server receives an RPC request, it forwards it to the cluster leader.

---
name: Introduction-to-Consul-Multi-DC
Introduction to Consul - Multi-DC
-------------------------
<br><br>
<img align="right" width="40%" src="images/consul_ref_arch.png">

* Gossip over a WAN connection is also possible
* Allows for request from one datacenter to be forwarded to another
* This allows for service level DR
* This allows for geographical service request handling

???
The server agents also operate as part of a WAN gossip pool. This pool is different from the LAN pool as it is optimized for the higher latency of the internet and is expected to contain only other Consul server agents. The purpose of this pool is to allow datacenters to discover each other in a low-touch manner. When a server receives a request for a different datacenter, it forwards it to a random server in the correct datacenter. That server may then forward to the local leader, so cross-datacenter requests are relatively fast and reliable.

---
name: Introduction-to-Consul-Protocols
Introduction to Consul - Protocols
-------------------------
We will dive deeper into Consul's two primary protocols in the next sections:
* Consensus
* Gossip

---
name: Section-Break-Consensus
class: middle, center

# Consensus


---

name: Introduction-to-Consensus
Introduction to Consensus
-------------------------

Consul uses a consensus protocol to provide consistency.
It is not necessary to understand consensus in detail, but you below are a few terms you will find useful when learning about Consul.

* **Log** - The primary unit of work in a Raft system is a log entry.
* **FSM (Finite State Machine)** - An FSM is a collection of finite states with transitions between them.
* **Peer set** - The peer set is the set of all members participating in log replication.
* **Quorum** - A quorum is a majority of members from a peer set.
* **Committed Entry** - An entry is considered committed when it is durably stored on a quorum of nodes.
* **Leader** - At any given time, the peer set elects a single node to be the leader.

There is a helpful visualization on the next slide.

---
name: Consensus-Visualization
Consensus - A Visualization
-------------------------
<br><br><br><br>
### .center[
<a href="http://thesecretlivesofdata.com/raft/" target="_blank">Raft Consensus Visualization</a>
]

---
name: Consensus-Modes
Consensus - Consistency Modes
-------------------------

While it's not required you understand Consensus under the hood, you should understand the various  consistency  modes so you can optimize for your workload.

* **Default** - Raft makes use of leader leasing, providing a time window in which the leader assumes its role is stable. However, if the old leader services any reads, the values are potentially stale. We make this trade-off because reads are fast, usually strongly consistent, and only stale in a hard-to-trigger situation.

* **Consistent** - This mode is strongly consistent without caveats. It requires that a leader verify with a quorum of peers that it is still leader. This introduces an additional round-trip to all server nodes. The trade-off is always consistent reads but increased latency due to the extra round trip.

* **Stale** - This mode allows any server to service the read regardless of whether it is the leader. This means reads can be arbitrarily stale but are generally within 50 milliseconds of the leader. The trade-off is very fast and scalable reads but with stale values. This mode allows reads without a leader meaning a cluster that is unavailable will still be able to respond.

---
name: Consensus-Deployment-Table
Consensus - Deployment Table
-------------------------

<br><br><br>
<center>
<table class="tg" width=60%>
  <tr>
    <th class="tg-feht">Servers</th>
    <th class="tg-feht">Quorum Size</th>
    <th class="tg-feht">Failure Tolerance</th>
  </tr>
  <tr>
    <td class="tg-3z1b">1</td>
    <td class="tg-3z1b">1</td>
    <td class="tg-3z1b">0</td>
  </tr>
  <tr>
    <td class="tg-2i6h">3</td>
    <td class="tg-2i6h">2</td>
    <td class="tg-2i6h">1</td>
  </tr>
  <tr>
    <td class="tg-3z1b">5</td>
    <td class="tg-3z1b">3</td>
    <td class="tg-3z1b">2</td>
  </tr>
</table>
</center>

This table illustrates the quorum size and failure tolerance for various cluster sizes. The recommended deployment is either 3 or 5 servers. A single server deployment is highly discouraged except for development, as data loss is inevitable in a failure scenario. Wherever possible servers should be located in separate low-latency failure zones.

???
Then, shalt thou count to three. No more. No less. Three shalt be the number thou shalt count, and the number of the counting shall be three. Four shalt thou not count, nor either count thou two, excepting that thou then proceed to three. Five is right out. 

---
name: Section-Break-Consensus
class: middle, center

# Gossip

---
name: Introduction-to-Gossip
Introduction to Gossip
-------------------------
Consul uses a gossip protocol to manage membership and broadcast messages to the cluster. All of this is provided through the use of the Serf library. The gossip protocol used by Serf is based on "SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol", with a few minor adaptations.

You can read more about Serf <a href="https://www.serf.io/docs/internals/gossip.html" target="_blank">here</a>.

Consul gossip operates two primary pools:
* LAN
* WAN

---
name: Introduction-to-Gossip-LAN-Pool
Introduction to Gossip - LAN Pool
-------------------------

* LAN pool is the smallest atomic unit of a datacenter
* Membership in a LAN pool facilitates the following : 
  * Automatic server discovery
  * Distributed failure detection
  * Reliable and fast event broadcasts

???
Each datacenter Consul operates in has a LAN gossip pool containing all members of the datacenter, both clients and servers. The LAN pool is used for a few purposes. Membership information allows clients to automatically discover servers, reducing the amount of configuration needed. The distributed failure detection allows the work of failure detection to be shared by the entire cluster instead of concentrated on a few servers. Lastly, the gossip pool allows for reliable and fast event broadcasts.

---
name: Introduction-to-Gossip-WAN-Pool
Introduction to Gossip - WAN Pool
-------------------------
* A WAN pool is a combination of several consul datacenters join via a WAN link
* Only the consul server nodes participate in the WAN pool
* Information is shared between the consul servers allowing for cross datacenter requests
* Just like in the LAN pool the WAN pool allows for graceful loss of an entire datacenter

???
The WAN pool is globally unique, as all servers should participate in the WAN pool regardless of datacenter. Membership information provided by the WAN pool allows servers to perform cross datacenter requests. The integrated failure detection allows Consul to gracefully handle an entire datacenter losing connectivity, or just a single server in a remote datacenter.

---
name: Introduction-to-Gossip-Visualization-50-Node
Introduction to Gossip - Visualization
-------------------------
.center[![:scale 40%](images/gossip_50_node.png)]
.center[50 nodes, ~3.56 gossip cycles] <br>

Gossip in Consul scales logarithmically, so it takes O(logN) rounds in order to reach all nodes.
For a 50 node cluster, we can estimate roughly 3.56 cycles to reach all the nodes.


---
name: Introduction-to-Gossip-Visualization-100-Node
Introduction to Gossip - Visualization
-------------------------
.center[![:scale 40%](images/gossip_100_node.png)]
.center[100 nodes, ~4.19 gossip cycles] <br>

For a 100 node clusters, this means roughly 4.19 cycles to reach all nodes. Pretty cool!
Let's look at this for  a large cluster.

---
name: Introduction-to-Gossip-Convergence
Introduction to Gossip - Convergence
-------------------------
.center[![:scale 80%](images/convergence_10k.png)]
.center[10k nodes, ~2 second convergence] <br>

The graph above shows the expected time to reach various states of convergence based on a 10k node cluster. We can converge on almost 100% of the nodes in just two seconds!

---
name: Introduction-to-Gossip-Skeptical
Introduction to Gossip - Skeptical ?
-------------------------
.center[![:scale 60%](images/mitchell_tweet.png)]