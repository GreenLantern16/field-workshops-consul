slug: service-mesh-with-consul-k8s
id: ygz7z7sj3nyn
type: track
title: Service mesh with Consul K8s
teaser: Unlesh the full power of Consul's service mesh on the world's most popular
  container orchestrator.
description: |-
  A long description of the track.

  You can use any GitHub flavoured markdown.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- lance@hashicorp.com
private: false
published: true
challenges:
- slug: deploy-connect
  id: iyea2sohruxq
  type: challenge
  title: Deploy Connect
  teaser: Your first Consul cluster on K8s
  assignment: |-
    Let's use Helm to push Consul into the K8s cluster, and enable Connect. <br>

    We've pulled the helm chart down from Git placed it in the lab environment.
    Some defaults for you that you can see in the code editor. <br>

    Let's go ahead and install Consul into K8s. Helm has been pre-initialized in this environment. <br>

    **You may see this error**: `Error: could not find a ready tiller pod`.
    Not a problem, just wait few moments, and try again. <br>

    ```
    helm install -f consul-vaules.yaml --name lab ./consul-helm
    ```

    The Consul UI will become available once the Consul server pods are running. <br>

    We've also included the K8s dashboard for the duration of this lab if you want to introspect workloads using the K8s UI.
    You can use the K8s token to authenticate to the dashboard. <br>

    If you want to see a breakout of Consul deployment, run the following command. <br>

    ```
    helm status lab
    ```

    Now we can deploy some apps.
  notes:
  - type: text
    contents: |-
      Connect can be used with Kubernetes to secure pod communication with other pods and external Kubernetes services.
      The Connect sidecar running Envoy can be automatically injected into pods in your cluster, making configuration for Kubernetes automatic. <br>

      This functionality is provided by the consul-k8s project and can be automatically installed and configured using the Consul Helm chart.
      You can read more about Helm [here](https://helm.sh/).<br>

      We recommend running Consul on Kubernetes with the same general architecture as running it anywhere else.
      There are some benefits Kubernetes can provide that eases operating a Consul cluster and Connect mesh that we will explore.
      The standard production deployment guide is still an important read even if running Consul within Kubernetes. <br>

      We use a lightweight distro of K8s for instruction in this lab called [k3s](https://k3s.io/).
      Your environment consists of one single node cluster of K8s.
      We will expose services over [NodePorts](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) in our lab for simplicity.
      This includes access to the Consul API on our K8s workstation.
  - type: image
    url: https://d33wubrfki0l68.cloudfront.net/949e085caf846f7e512f420bcbd0d1a2935e27bb/4c93c/static/img/k8s-consul-simple.png
  tabs:
  - title: Helm Config
    type: code
    hostname: kubernetes
    path: /root/consul-vaules.yaml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s - UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: K8s - Token
    type: code
    hostname: kubernetes
    path: /root/token.txt
  difficulty: basic
  timelimit: 300
- slug: deploy-application
  id: 71ekxt1ehjg5
  type: challenge
  title: Deploy Application
  teaser: A short description of the challenge.
  assignment: |-
    Let's start by investigating our config for the Emojify app in the code editor.
    You'll notice each of our services has the following annotations.

    ```
    consul.hashicorp.com/connect-inject
    consul.hashicorp.com/connect-service-upstreams
    ```

    These annotations tell Consul that the pod should get an Envoy sidecar to route traffic,
    and listeners for the requested service on the specified upstream port.
    If you recall our helm chart, we set the following value, so we have to be explicit with our deployment files.<br>

    ```
    connectInject:
      default: false
    ```

    We can deploy our application with the following command.

    ```
    kubectl apply -f emojify/ingress.yml
    kubectl apply -f emojify/website_v1.yml
    kubectl apply -f emojify/api.yml
    kubectl apply -f emojify/cache.yml
    kubectl apply -f emojify/facebox.yml
    ```

    We have a deny by default rule in place that is disallowing all traffic.
    Let's go ahead and update our intentions so our microservices can serve traffic. <br>

    ```
    consul intention create -allow emojify-api emojify-facebox
    consul intention create -allow emojify-api emojify-cache
    consul intention create -allow emojify-ingress emojify-api
    consul intention create -allow emojify-ingress emojify-website
    ```

    Now you should be able to access the website through our Nginx ingress container!
  notes:
  - type: text
    contents: |-
      In this lab, we will deploy a more complex microservice application called [Emojify](https://github.com/emojify-app). <br>

      The application has five distinct components that provide functionality:

      * Ingress - Nginx container that gives us access to the API & Website.
      * Website - Serves static content for the Emojify website.
      * API - Provides API to machine learning backend.
      * Cache - Cache layer for API.
      * Facebox - Provides machine learning for detecting and identifies faces in photos.

      In this section we'll explore how these services can be easily connected, monitored, and scaled with Connect. <br>

      Consul Connect supports more advanced ingress controllers, such as [Datawire's Amabassador](https://www.consul.io/docs/platform/k8s/ambassador.html),
      but we will use the Nginx container in this lab to show basic functionality.
  tabs:
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/dc1/intentions
    port: 30085
  - title: K8s - UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: K8s - Token
    type: code
    hostname: kubernetes
    path: /root/token.txt
  difficulty: basic
  timelimit: 600
- slug: emojify-some-faces
  id: ude8vm6frk3v
  type: challenge
  title: Emojify some Faces
  teaser: A short description of the challenge.
  assignment: |-
    We can enter a URL into our website to `emojify` the image. If you don't have an image you'd like to use try the one below of our founders.

    ```
    https://i.ytimg.com/vi/PP8-TRQeC_4/maxresdefault.jpg
    ```

    Not perfect, but pretty close! <br>

    Let's take a look at the service defintion Consul was able to generate based on our K8s annotations.
    We've played it in the editor tab for you. You can also view it below. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- cat  /consul/connect-inject/service.hcl
    ```

    You might have remembered creating those Consul service definitions by hand.
    In K8s, all we needed was the following to generate all the necessary configurations.

    ```
    template:
      metadata:
        labels:
          app: emojify-api
        annotations:
          "consul.hashicorp.com/connect-inject": "true"
          "consul.hashicorp.com/connect-service-upstreams": "emojify-facebox:8003,emojify-cache:8005"
    ```

    Well done! Now we can move to more advanced use cases.
  notes:
  - type: text
    contents: |-
      Now that our app is working we can Emojify some faces.
      Feel free to use your colleagues, or our sample. <br>
  tabs:
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Api -  Service
    type: code
    hostname: kubernetes
    path: /tmp/api-service.hcl
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s - UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: K8s - Token
    type: code
    hostname: kubernetes
    path: /root/token.txt
  difficulty: basic
  timelimit: 300
- slug: scale-the-app
  id: r2vgop1q59ix
  type: challenge
  title: Scale the App
  teaser: A short description of the challenge.
  assignment: |-
    Let's revisit the emojify app specs from eariler, and scale up the number of Facebook services providing our ML capabilities. <br>

    Go in the code editor and change the number of replicas for the facebook service to `2`.

    ```
    spec:
      replicas: 2
    ```

    Now we can scale up the service.

    ```
    kubectl apply -f emojify/facebox.yml
    ```

    Now we can verify in the Consul catalog that we have two healthy services. You can also do this in the Consul UI.

    ```
    curl -s localhost:30085/v1/catalog/service/emojify-facebox | jq '[.. |."ServiceAddress"? | select(. != null)]'
    ```

    We can do the same validation for our Envoy sidecar proxy.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters | grep facebox
    ```

    Nice work! You just leveraged dyanmic service discovery inside your service mesh to scale out a workload.
  notes:
  - type: text
    contents: |-
      Now that we've seen the power of Emojify, users are flocking to the application! <br>

      In this assignment we'll scale up our backend services and preserve continuity in the mesh.
  tabs:
  - title: Facebox - Config
    type: code
    hostname: kubernetes
    path: /root/emojify/facebox.yml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s - UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: K8s - Token
    type: code
    hostname: kubernetes
    path: /root/token.txt
  difficulty: basic
  timelimit: 300
- slug: get-metrics
  id: hwu25pwhbljf
  type: challenge
  title: Get Metrics
  teaser: Please observe...
  assignment: |-
    You can see pods running with Grafana and Prometheus for our monitoring stack.
    We've installed them for you via their respective helm charts. <br>

    ```
    * [Grafana](https://github.com/helm/charts/tree/master/stable/grafana)
    * [Prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)
    ```

    We have a sample dashboard built for you that can visuliaze metrics for our application stack.
    Follow [these instructions](https://grafana.com/docs/reference/export_import/#importing-a-dashboard) from Grafana to import your dashboard.<br>

    ```
    https://raw.githubusercontent.com/hashicorp/consul-k8s-l7-obs-guide/master/overview_dashboard.json
    ```

    We have a application that can simulate traffic for us. Let's run it now. <br>

    ```
    kubectl apply -f emojify/traffic.yml
    ```

    Wait a few moments, and check your dashbaord. Metrics will be streaming in shortly. Nice work!
  notes:
  - type: text
    contents: |-
      In the last assignment we scaled up a component of our application,
      but how would we have made that determination in the first place? <br>

      In this assignment we'll look at detailed telemetry for our microservices.
      This observability is a powerful feature of Consul Connect and gives us real-time insights on our application performance. <br>

      We're spinning up a monitoring stack for you.
      Please be patient. This should take 1-2 minutes.
  tabs:
  - title: Grafana - UI
    type: service
    hostname: kubernetes
    port: 30030
  - title: Grafana - Password
    type: code
    hostname: kubernetes
    path: /tmp/grafana-pass.txt
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s - UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: K8s - Token
    type: code
    hostname: kubernetes
    path: /root/token.txt
  difficulty: basic
  timelimit: 300
- slug: ship-a-new-feature
  id: w95srj56cyzm
  type: challenge
  title: Ship a New Feature
  teaser: A short description of the challenge.
  assignment: |-
    We have a few more files in our `emojify` folder.
    The new service we introduce will allow users to optionally purchase their photograph after its emojfied. <br>

    You'll notice in our deployment spec for the `website` we used a Consul defintion called `consul.hashicorp.com/service-tags`.
    This metadata will allow us send traffic  to either `v1` or `v2` of the website based on certain conditions. <br>

    Let's start by pushing Connect's L7 routing config for this service. Each of these configurations does a few things for us. <br>

    * Resolver - Creates subsets of the website service. In our example, we use the service tags.

    ```
    kind           = "service-resolver"
    name           = "emojify-website"
    default_subset = "v1"
    subsets = {
      "v1" = {
        filter = "v1 in Service.Tags"
      }
      "v2" = {
        filter = "v2 in Service.Tags"
      }
    }
    ```

    * Spliter - Contains our traffic shaping rules. We're pretty confident in our app, so we're going to send 80% of the traffic to the new service.

    ```
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 20
        service_subset = "v1"
      },
      {
        weight         = 80
        service_subset = "v2"
      },
    ]
    ```

    * Router   - Allows us to do some version specific testing by supplying query params.

    ```
    kind = "service-router"
    name = "emojify-website"
    routes = [
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "1"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v1"
        }
      },
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "2"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v2"
        }
      }
    ]
    ```

    Now let's apply them. <br>

    ```
    consul config write emojify/resolver.hcl
    consul config write emojify/splitter.hcl
    consul config write emojify/router.hcl
    ```

    With our new routing rules in place, let's go ahead and push the new version of the website with the payment service.

    ```
    kubectl  apply -f emojify/website_v2.yml
    kubectl  apply -f emojify/payments.yml
    consul intention create -allow emojify-ingress emojify-payments
    ```

    Once your pods are healthy, we can start our testing.
    Let's use that header value we set up eariler to look at our different versions of t he website config.
    Pay attention to the `PAYMENT_ENABLED` value. You can also see this in the `v1` and `v2` tabs.

    ```
    curl localhost:30000/config/env.js?x-version=1
    curl localhost:30000/config/env.js?x-version=2
    ```

    Now, refresh the app a few times, and look for `EmojifyEnterprise` in the top left banner.
    This is the v2 version of our website. <br>

    Emojify another image, and you will see a new option to purchase the image.
    Fill in some dummy details and try it now. <br>

    It looks like you don't have enough funds (don't worry, this happens every time), better call your bank! <br>

    You'll notice that Envoy is now  grouping clusters by our subset.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters  | grep emojify-website
    ```

    You can also see Envoy has updated our route definitions for our debug params and traffic weights. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."routes"? | select(. != null)][-1]'
    ```

    Optionally, you can shift all the traffic to the v2 version of the website now that our new feature is working properly.
    You can additionally verify Consul has propgated this change to envoy by running the above command again.

    ```
    cat <<-EOF > /root/emojify/splitter.hcl
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 0
        service_subset = "v1"
      },
      {
        weight         = 100
        service_subset = "v2"
      },
    ]
    EOF
    consul config write emojify/splitter.hcl
    ```

    Nice work! you just used traffic shaping to safely move users to a new version of your application!
  notes:
  - type: text
    contents: |-
      Now that we scaled our app, and used some metrics to determine it's stable, we can ship a new feature of our application.

      Let's use Connect's L7 routing capabilities to a/b test our application before we roll out to all our users.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: Emojify - A/B
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s - UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: K8s - Token
    type: code
    hostname: kubernetes
    path: /root/token.txt
  difficulty: basic
  timelimit: 300
checksum: "3940354818981822214"
