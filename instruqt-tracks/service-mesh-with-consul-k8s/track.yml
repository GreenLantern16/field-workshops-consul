slug: service-mesh-with-consul-k8s
id: ygz7z7sj3nyn
type: track
title: Service mesh with Consul K8s
teaser: Unleash the full power of Consul's service mesh on the world's most popular
  container orchestrator.
description: |-
  In this track you will use Consul Connect's first-class support on K8s
  for service mesh.

  You'll work through scaling, monitoring, and tracing applications. To complete the track you, will gain experience with the advanced L7 traffic patterns.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- lance@hashicorp.com
private: false
published: true
challenges:
- slug: deploy-connect
  id: iyea2sohruxq
  type: challenge
  title: Deploy Connect
  teaser: Your first Consul cluster on K8s
  assignment: |-
    First, use Helm with a custom values file to deploy Consul into the K8s cluster and enable Connect. <br>

    The helm chart has been cloned from GitHub and placed it in your lab environment.
    Click on the `/root/consul-valules.yaml` file to view the Consul deployment configuration in the code editor. <br>

    Navigate to the `_K8s` shell to install Consul. Note, Helm has been pre-initialized in this environment. Copy or type the following command in the shell.<br>

    ```
    helm install -f consul-values.yaml lab ./consul-helm --wait --debug
    ```

    The status output should indicate Consul was `DEPLOYED`. <br>


    The Consul UI will become available once the Consul server pods are running. Monitor the progress with the following `kubectl` command. If `lab-consul-server-0` is `running`, then you should be able to access the Consul UI <br>

    ```
    kubectl get pods
    ```

    <br>

    Now that you have successfully deployed Consul on K8s, you can deploy some apps! To continue, click the `Check` button.

    **Optional**

    We've included the K8s dashboard for the duration of this lab if you want to compare the workloads using the K8s UI tab.
    Note, you can use  the `skip` button to authenticate, since the dashboard is already configuration. <br>

    If you want to see a breakout of the Consul deployment from helm, run the following command. <br>

    ```
    helm status lab
    ```
  notes:
  - type: text
    contents: |-
      Connect can be used with Kubernetes to secure pod communication with other pods and external Kubernetes services.
      The Connect sidecar proxy running Envoy can be automatically injected into pods in your cluster, making configuration for Kubernetes automatic. <br>

      This functionality is provided by the consul-k8s project and can be automatically installed and configured using the official Consul Helm chart.
  - type: text
    contents: |-
      We recommend running Consul on Kubernetes with the same general architecture as running it anywhere else.
      There are some benefits Kubernetes can provide that eases operating a Consul cluster and Connect mesh that we will explore.
      The standard production [deployment guide](https://learn.hashicorp.com/consul/datacenter-deploy/deployment-guide?utm_source=instruqt&utm_medium=k8s-track&utm_term=dg) is still an important read even if running Consul within Kubernetes. <br>

      We use a lightweight distro of K8s for this lab called [k3s](https://k3s.io/).
      Your environment consists of a single node cluster of K8s.
      We will expose services over [NodePorts](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) in our lab for simplicity.
      This includes access to the Consul API on our K8s workstation.
  - type: image
    url: https://d33wubrfki0l68.cloudfront.net/949e085caf846f7e512f420bcbd0d1a2935e27bb/4c93c/static/img/k8s-consul-simple.png
  tabs:
  - title: Helm Config
    type: code
    hostname: kubernetes
    path: /root/consul-values.yaml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: deploy-application
  id: 71ekxt1ehjg5
  type: challenge
  title: Deploy the Application
  teaser: Bring our application to the world.
  assignment: |-
    Before deploying your apps, investigate your config for the Emojify app in the code editor called `Emojify-Config`.
    Select any of the YAML files, you'll notice the service has the following annotations.

    ```
    consul.hashicorp.com/connect-inject
    consul.hashicorp.com/connect-service-upstreams
    ```

    If you view any of the other services, they will include the same annotations. <br>

    These annotations tell Consul that the pod should get an Envoy sidecar proxy to route traffic
    and listeners for the requested service on the specified upstream port.
    In the helm chart, you set  `connectInject` to `false`, so now you need to explicitly add the above annotations to deploy a sidecar proxy with your application.<br>

    ```
    connectInject:
      default: false
    ```

    Now, you can deploy your applications. Navigate back to the `_K8s` shell and execute the following commands.

    ```
    kubectl apply -f emojify/ingress.yml
    kubectl apply -f emojify/website_v1.yml
    kubectl apply -f emojify/api.yml
    kubectl apply -f emojify/cache.yml
    kubectl apply -f emojify/facebox.yml

    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-ingress -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-website -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-api -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-cache -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-facebox -o name)
    ```

    By default, the Consul cluster has been configured to deny all traffic between applications. A deny all by default configuration is best practice. You should then allow traffic between applications using intentions.
    Create the following four intentions so your microservices can serve traffic. <br>

    ```
    consul intention create -allow emojify-api emojify-facebox
    consul intention create -allow emojify-api emojify-cache
    consul intention create -allow emojify-ingress emojify-api
    consul intention create -allow emojify-ingress emojify-website
    ```

    Now you've deployed your applications and created allow intentions, you should be able to access the website on the `Emojify-Website` tab.
  notes:
  - type: text
    contents: |-
      In this exercise, you will deploy a more complex microservice application called [Emojify](https://github.com/emojify-app). <br>

      The application has five distinct components that provide functionality:

      * Ingress - Nginx container that gives us access to the API & Website.
      * Website - Serves static content for the Emojify website.
      * API - Provides API to machine learning backend.
      * Cache - Cache layer for API.
      * Facebox - Provides machine learning for detecting and identifies faces in photos.

      In this exercise you'll also explore how these services can be easily connected, monitored, and scaled with Connect. <br>

      Consul Connect supports more advanced ingress controllers, such as [Datawire's Ambassador](https://www.consul.io/docs/platform/k8s/ambassador.html),
      but we will use the Nginx container in this lab to show basic functionality.
  tabs:
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/dc1/intentions
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  difficulty: basic
  timelimit: 600
- slug: use-the-application
  id: ibdhfvn7xzbo
  type: challenge
  title: Use the Application
  teaser: You get an emoji, and you get an emoji, and you get an emoji.
  assignment: |-
    To use your Emojify app, enter a URL into the website to "emojify" the image. If you don't have an image you'd like to use try the one below of our founders.

    ```
    https://cdn.geekwire.com/wp-content/uploads/2017/10/armon-dadgar-and-mitchell-hashimoto-630x419.jpg
    ```

    Next, in the `Api-Service` tab, view the Consul service definition generated based on your K8s YAML files.
    You can also discover the service definition using `kubectl`. In the `_K8s` shell tab, execute the command below. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- cat  /consul/connect-inject/service.hcl
    ```

    If you have previous Consul experience, you have practice creating service definitions by hand.
    In K8s, all you needed is the following to generate all the necessary configurations.

    ```
    template:
      metadata:
        labels:
          app: emojify-api
        annotations:
          "consul.hashicorp.com/connect-inject": "true"
          "consul.hashicorp.com/connect-service-upstreams": "emojify-facebox:8003,emojify-cache:8005"
    ```

    Now that you've tested your Emojify app, you can move to more advanced use cases!
  notes:
  - type: text
    contents: |-
      Now that your app is working, you can Emojify some faces.
      Feel free to use your colleagues, or our sample. <br>
  tabs:
  - title: Api -  Service
    type: code
    hostname: kubernetes
    path: /tmp/api-service.hcl
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  difficulty: basic
  timelimit: 600
- slug: scale-the-app
  id: r2vgop1q59ix
  type: challenge
  title: Scale the Application
  teaser: Handle increased demand
  assignment: |-
    To scale the number of Facebox services providing our ML capabilities, increase the `replicas`. <br>

    In the emojify app specs from earlier, change the number of replicas for the Facebox service to `2`. Note, save the changes by clicking the "save" icon next to the file name.

    ```
    spec:
      replicas: 2
    ```

    Next, use `kubectl` to update the spec.

    ```
    kubectl apply -f emojify/facebox.yml
    ```

    Finally, verify the Consul catalog has two healthy services. You can also do this in the `Consul UI` tab or by using the HTTP API command below in the `_K8s` shell.

    ```
    curl -s localhost:30085/v1/catalog/service/emojify-facebox | jq '[.. |."ServiceAddress"? | select(. != null)]'
    ```

    You should also verify Envoy sidecar proxy is healthy using `kubectl`.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters | grep facebox
    ```

    Nice work! You just leveraged dynamic service discovery inside your service mesh to scale out a workload.
  notes:
  - type: text
    contents: |-
      Now that users have seen the power of Emojify, they are flocking to the application! <br>

      In the next exercise you'll scale your backend services and preserve continuity in the mesh.
  tabs:
  - title: Facebox - Config
    type: code
    hostname: kubernetes
    path: /root/emojify/facebox.yml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  difficulty: basic
  timelimit: 600
- slug: get-metrics
  id: hwu25pwhbljf
  type: challenge
  title: Get Metrics
  teaser: Please observe
  assignment: |-
    Your monitoring stack includes Grafana and Prometheus. Ensure both applications have healthy, running pods using `kubectl`. <br>

    ```
    kubectl get pod --selector=app=prometheus
    kubectl get pod --selector=app=grafana
    ```

    They were installed via their respective helm charts.

    * [Grafana](https://github.com/helm/charts/tree/master/stable/grafana)
    * [Prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)

    You can log into Grafana in the UI tab with the below credentials:

    * username: `admin`
    * password: `check the password tab`

    There is a sample dashboard built for you that can visualize metrics for your application stack. <br>

    To create the sample dashboard select the `+` in the left side navigation to `Import`.
    You will need to paste the raw json into Grafana per the above documentation. <br>

    ```
    https://raw.githubusercontent.com/hashicorp/consul-k8s-l7-obs-guide/master/overview_dashboard.json
    ```

    If you encounter issues, you can follow [the complete instructions](https://grafana.com/docs/reference/export_import/#importing-a-dashboard) from Grafana to import your own dashboard.


    You have a application that can simulate traffic for you. Let's run it now in the `_K8s` shell. <br>

    ```
    kubectl apply -f emojify/traffic.yml
    ```

    Wait a few moments and check your dashboard. Metrics will be streaming in shortly. Nice work!
  notes:
  - type: text
    contents: |-
      In the previous exercise you scaled up a component of your application,
      but how would you have made that determination in the first place? <br>

      In this exercise you'll review detailed telemetry for your microservices.
      This observability is a powerful feature of Consul Connect and gives you real-time insights on our application performance. <br>

      We're spinning up a monitoring stack for you.
      Please be patient. This should take 1-2 minutes.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Grafana - UI
    type: service
    hostname: kubernetes
    port: 30030
  - title: Grafana - Password
    type: code
    hostname: kubernetes
    path: /tmp/grafana-pass.txt
  - title: Prometheus - UI
    type: service
    hostname: kubernetes
    path: /targets#job-kubernetes-pods
    port: 30090
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: ship-a-new-feature
  id: w95srj56cyzm
  type: challenge
  title: Ship a New Feature
  teaser: Selectively bring users to a new version of the application.
  assignment: |-
    In this exercise, you will use Connect's traffic splitting feature for an a/b deployment. <br>

    Notice, you have a few more files in our `emojify` folder.
    The new service will allow users to optionally purchase their photograph after its emojfied. <br>

    You'll notice in the deployment spec for the `website`, the Consul definition called `consul.hashicorp.com/service-tags`.
    This metadata will allow you to send traffic to either `v1` or `v2` of the website based on certain conditions. <br>

    Start by deploying Connect's L7 routing config for this service. Each of these configurations do a few things. <br>

    * Resolver - Creates a subsets of the website service. In this example, you will use the service tags.

    ```
    kind           = "service-resolver"
    name           = "emojify-website"
    default_subset = "v1"
    subsets = {
      "v1" = {
        filter = "v1 in Service.Tags"
      }
      "v2" = {
        filter = "v2 in Service.Tags"
      }
    }
    ```

    * Splitter - Contains your traffic shaping rules. Since you're pretty confident in your app,
    you'll send 90% of the traffic to the new service.

    ```
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 10
        service_subset = "v1"
      },
      {
        weight         = 90
        service_subset = "v2"
      },
    ]
    ```

    * Router   - Allows you to do some version specific testing by supplying query params.

    ```
    kind = "service-router"
    name = "emojify-website"
    routes = [
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "1"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v1"
        }
      },
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "2"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v2"
        }
      }
    ]
    ```

    Next, apply the routing config. <br>

    ```
    consul config write emojify/resolver.hcl
    consul config write emojify/splitter.hcl
    consul config write emojify/router.hcl
    ```

    With your new routing rules in place, go ahead and push the new version of the website with the payment service.

    ```
    kubectl  apply -f emojify/website_v2.yml
    kubectl  apply -f emojify/payments.yml

    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-payments -o name)

    consul intention create -allow emojify-ingress emojify-payments
    ```

    Once your pods are healthy, you can start our testing.
    Check Consul for the `Website` service and look at the `ServiceTags` value.
    We should see two tagged versions of the application using the HTTP API. <br>

    ```
    curl -s http://127.0.0.1:8500/v1/catalog/service/emojify-website | jq
    ```

    Use that header value you set up earlier to look at our different versions of the website config.
    Pay attention to the `PAYMENT_ENABLED` value. <br>

    ```
    curl localhost:30000/config/env.js?x-version=1
    curl localhost:30000/config/env.js?x-version=2
    ```

    Now, refresh the app a few times and look for `EmojifyEnterprise` in the top left banner.
    This is the v2 version of our website. <br>

    Emojify another image and you will see a new option to purchase the image.
    Fill in some dummy details and try it now. <br>

    It looks like you don't have enough funds (don't worry, this happens every time), better call your bank! <br>

    Notice that Envoy is now grouping clusters by your subsets.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters  | grep emojify-website
    ```

    You can also see Envoy has updated your route definitions for your debug params and traffic weights. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."routes"? | select(. != null)][-1]'
    ```

    Finally, you can shift all the traffic to the v2 version of the website now that your new feature is working properly.

    ```
    cat <<-EOF > /root/emojify/splitter.hcl
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 0
        service_subset = "v1"
      },
      {
        weight         = 100
        service_subset = "v2"
      },
    ]
    EOF
    consul config write emojify/splitter.hcl
    ```

    You can additionally verify Consul has propagated this change to Envoy by running the above command again. <br>

    Nice work! you just used traffic splitting to safely move users to a new version of your application!
  notes:
  - type: text
    contents: |-
      Now that you've scaled the app and used some metrics to determine it's stable, you can ship a new feature.

      In the next exercise, you'll use Connect's L7 routing capabilities to a/b test your application before you roll out to all our users.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: Emojify - A/B
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 900
- slug: tracing
  id: 8cnsp3ybcrlc
  type: challenge
  title: Tracing Distributed Errors
  teaser: Down the rabbit hole we go.
  assignment: |-
    We've deployed a few Jaeger services for you while setting up this exercise.
    They can be seen be running the below command.
    The Jaeger UI is also exposed in the tab, you will use it shortly to visualize some traces. <br>

    ```
    kubectl get svc --selector=app=jaeger
    ```

    You will use the following Connect configuration to configure Envoy to route your trace information to your Jaeger instance.

    * [Cluster Bootstrap](https://www.consul.io/docs/connect/proxies/envoy.html#envoy_extra_static_clusters_json)
    * [Tracing Bootstrap](https://www.consul.io/docs/connect/proxies/envoy.html#envoy_tracing_json)

    ```
      {
        "connect_timeout": "3.000s",
        "dns_lookup_family": "V4_ONLY",
        "lb_policy": "ROUND_ROBIN",
        "load_assignment": {
            "cluster_name": "jaeger_9411",
            "endpoints": [
                {
                    "lb_endpoints": [
                        {
                            "endpoint": {
                                "address": {
                                    "socket_address": {
                                        "address": "jaeger-collector",
                                        "port_value": 9411,
                                        "protocol": "TCP"
                                    }
                                }
                            }
                        }
                    ]
                }
            ]
        },
        "name": "jaeger_9411",
        "type": "STRICT_DNS"
      }
    ```

    ```
      {
        "http": {
          "name": "envoy.zipkin",
          "config": {
            "collector_cluster": "jaeger_9411",
            "collector_endpoint": "/api/v1/spans",
            "shared_span_context": false
          }
        }
      }
    ```

    Go ahead and apply the config.

    ```
    consul config write trace.hcl
    ```

    Now you can deploy the tracing application and configure your intentions.

    ```
    kubectl apply -f tracing

    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=web -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=api -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=cache -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=payments -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=currency -o name)

    consul intention create -allow web api
    consul intention create -allow api cache
    consul intention create -allow api payments
    consul intention create -allow payments currency
    ```

    Once the pods are running, you can test the application through the `Frontend`, which is exposed over a NodePort.
    It has been configured to have a 50% error rate for one of the backend services, so run this command twice.

    ```
    curl localhost:30900 | jq
    curl localhost:30900 | jq
    ```

    Navigate to the Jaeger UI and check out the traces for the last two API requests.
    For the failed API request, it is easy to determine where it occurred. <br>

    Trace data sent by Envoy will have a `component` span tag with a value of `proxy`.
    You can see a detailed description of all the data Envoy sends [here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing#what-data-each-trace-contains). <br>

    The other spans are instrumented with the application client library for tracing.
    Compare the differences in the span tags and logs. Notice the granularity of the log events instrumented by the application's client library. <br>

    We can do a quick validation that Consul was able to apply the tracing definition to our Envoy proxy config.
    Let's check this out for our API service.

    ```
    kubectl exec $(kubectl get pod --selector=app=api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."tracing"? | select(. != null)]'
    ```

    Nice work, you just debugged a distributed app with tracing.
  notes:
  - type: text
    contents: |-
      In this exercise you will take a look at Connect's tracing capabilities. <br>

      Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures.
      It can be invaluable in understanding serialization, parallelism, and sources of latency. <br>

      Envoy helps Connect do this in a few ways:

      * Generates request IDs and trace headers for requests as they flow through the proxies
      * Sends the generated trace spans to the tracing backends
      * Forwards the trace headers to the proxied application

      You'll explore this in this challenge using popular tracing solutions Zipkin and Jaeger. <br>
  - type: text
    contents: |-
      Connect sidecars will proxy both inbound and outbound requests, but it does not automatically know how to correlate them.
      This correlation is called `header propagation` and requires instrumentation at the application level. <br>

      Your sample app does this with `zipkin-go` libraries and the OpenTracing API.
      The application compliments the trace data sent by the proxies with useful span tags and logs for better traceability. <br>
  - type: text
    contents: |-
      You will use a purpose built app called `fake-service` to test your upstreams and trace these errors.
      The app will deploy five services in the following configuration: <br>

      * Frontend - Access to our application
      * API - gRPC API to backend services
      * Cache - Cache responses for our API
      * Payments  - Process payments
      * Currency - Do currency lookups for our payments

      The application code can be found here: https://github.com/nicholasjackson/fake-service
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: App UI
    type: service
    hostname: kubernetes
    path: /ui
    port: 30900
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=web
    port: 31686
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 900
- slug: ingress
  id: ydjcyyarz7ef
  type: challenge
  title: Ingress
  teaser: Use advanced ingress capabilities.
  assignment: |-
    We've configured Ambassador for you in this exercise with the following K8s deployment files: <br>

    * https://www.getambassador.io/yaml/aes-crds.yaml
    * https://www.getambassador.io/yaml/aes.yaml
    * https://www.getambassador.io/yaml/consul/ambassador-consul-connector.yaml

    We've also enabled [Ambassador tracing](https://www.getambassador.io/user-guide/tracing-tutorial-zipkin/), so you can see the gateway ingress in the Jaeger UI. <br>

    You can follow this [guide](https://www.getambassador.io/user-guide/consul/) to see all the integrations with Consul. <br>

    For the integration to work, you need to set up an Ambassador resolver that can query Consul's catalog. <br>

    ```
    cat <<EOF | kubectl -n ambassador apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: ConsulResolver
    metadata:
      name: lab-consul-dc1
    spec:
      address: lab-consul-server.default.svc.cluster.local:8500
      datacenter: dc1
    EOF
    ```

    That's it! Ambassador is ready to go. Now you can create a mapping for the application. <br>

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: Mapping
    metadata:
      name: consul-web-mapping-tls
    spec:
      prefix: /web/
      service: web-sidecar-proxy
      resolver: lab-consul-dc1
      tls: ambassador-consul
      load_balancer:
        policy: round_robin
    EOF
    ```

    At this step, you can look at the route in the Ambassador UI tab and check its health. <br>

    We can also view all the mappings we've created, or a specific mapping.
    Alternatively, you can manage these [inline](https://www.getambassador.io/reference/mappings/#mapping-resources-and-crds). <br>

    ```
    kubectl get mappings
    kubectl describe mapping consul-web-mapping-tls
    ```

    Ambassador is also trusted in the mesh and has it's own certificate to present to the proxies.
    The integration manages this as a K8s tls secret. You can see this below. <br>

    ```
    kubectl get secrets -n ambassador ambassador-consul-connect -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -text -noout
    ```

    You need to tell Connect to allow this traffic. You can do this selectively on a per service basis. <br>

    ```
    consul intention create -allow ambassador web
    ```

    Remember, your ingress controller is sitting behind the K3s load balancer. <br>

    ```
    kubectl describe svc -n ambassador ambassador
    ```

    Last, let's set up end-user authentication with a [JWT filter](https://www.getambassador.io/reference/filter-reference/#filter-type-jwt).
    Typically with web applications, the authentication is implemented at the edge, either via an API/edge gateway or via a top-level request filter within your application framework.
    The mesh does not control these external clients, so we must authenticate them differently. <br>

    The filter is configured to require a `web` audience, and it will also inject the JWT token and pass it to the downstream applications in the mesh.
    We are using the [none algorithm](https://tools.ietf.org/html/rfc7518#section-3.6) for demo purposes.
    Apply it now. <br>


    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: Filter
    metadata:
      name: jwt-filter
    spec:
      JWT:
        jwksURI: "https://getambassador-demo.auth0.com/.well-known/jwks.json"
        validAlgorithms:
          - "none"
        requireAudience: yes
        audience: "web"
        injectRequestHeaders:
          - name: "X-Token-String"
            value: "{{ .token.Raw }}"
    EOF
    ```

    Next, configure the `/web/` route to use the filter.

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: FilterPolicy
    metadata:
      name: web-policy
    spec:
      rules:
        - host: "*"
          path: /web/
          filters:
            - name: jwt-filter
    EOF
    ```

    Finally, Ambassador is deployed with a self-signed certificate for end-to-end mTLS in the lab environment, so we'll ignore trust validation in the API call.
    Get the load balancer IP <br>

    ```
    ambassador=$(kubectl get svc ambassador -n ambassador -o json | jq -r '.status.loadBalancer.ingress[0].ip')
    openssl s_client -connect ${ambassador}:443 </dev/null 2>/dev/null | openssl x509 -text
    ```

    Now, send some traffic the application.
    A sample JWT has been provided for you. Decode it first, then try the API.  <br>

    ```
    jwt eyJhbGciOiJub25lIiwidHlwIjoiSldUIiwiZXh0cmEiOiJzbyBtdWNoIn0.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkxhbmNlIExhcnNlbiIsImlhdCI6MTU3Nzg4MDAwMCwgImF1ZCI6ICJ3ZWIifQ.
    ```

    Experiment with authorization, and without it. <br>

    ```
    curl -k https://${ambassador}/web/
    curl -k -H 'Authorization: Bearer eyJhbGciOiJub25lIiwidHlwIjoiSldUIiwiZXh0cmEiOiJzbyBtdWNoIn0.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkxhbmNlIExhcnNlbiIsImlhdCI6MTU3Nzg4MDAwMCwgImF1ZCI6ICJ3ZWIifQ.' https://${ambassador}/web/
    ```

    Log into the Jaeger UI and look at the new traces.
    You'll see additional traffic from Ambassador.  Make sure to check out the `ext_autz` traces.
    Nice work!! <br>
  notes:
  - type: text
    contents: |-
      Consul supports a rich ecosystem of integrations and Connect is no exception.
      In this exercise you'll leverage [Datawire's Ambassador](https://www.consul.io/docs/platform/k8s/ambassador.html)
      to allow K8s ingress for your tracing application from the prior exercise. <br>

      Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster.
      Ingress can be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, end-user authentication and more. <br>

      It can be very expensive to run dedicated load balancers per application.
      Ingress can help us manage this problem effectively. <br>

      An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer.
      We will use the [MetalLB load balancer](https://metallb.universe.tf/) to send Ambassador the traffic. <br>

      Try it out with Ambassador and Connect!
  tabs:
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=ambassador-ambassador
    port: 31686
  - title: Ambassador UI
    type: service
    hostname: kubernetes
    path: /ambassador/v0/diag/
    port: 31877
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  difficulty: basic
  timelimit: 300
- slug: summary
  id: neancacl0syl
  type: challenge
  title: Summary
  teaser: Sandbox mode
  assignment: |-
    Congratulations on completing this lab!!! The environment will be available for the next hour and a half.
    Click `check` when  you  are  done exploring to teardown the environment.
  notes:
  - type: text
    contents: The next assignment preserves the environment.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=ambassador-ambassador
    port: 31686
  - title: Ambassador UI
    type: service
    hostname: kubernetes
    path: /ambassador/v0/diag/
    port: 31877
  difficulty: basic
  timelimit: 5400
checksum: "13320107809007628379"
show_timer: true
