slug: service-mesh-with-consul-k8s
id: ygz7z7sj3nyn
type: track
title: Service mesh with Consul K8s
teaser: Unleash the full power of Consul's service mesh on the world's most popular
  container orchestrator.
description: |-
  In this track you will use Consul Connect's first-class support on K8s
  for service mesh.

  We'll work through scaling, monitoring, tracing, and advanced L7 traffic patterns.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- scarolan@hashicorp.com
- lance@hashicorp.com
private: false
published: true
challenges:
- slug: deploy-connect
  id: iyea2sohruxq
  type: challenge
  title: Deploy Connect
  teaser: Your first Consul cluster on K8s
  assignment: |-
    Let's use Helm to push Consul into the K8s cluster, and enable Connect. <br>

    We've pulled the helm chart down from Git placed it in the lab environment.
    Some defaults for you that you can see in the code editor. <br>

    Let's go ahead and install Consul into K8s. Helm has been pre-initialized in this environment. <br>

    ```
    helm install -f consul-values.yaml --name lab ./consul-helm
    ```
    **You may see this error**: `Error: could not find a ready tiller pod`.
    Not a problem, just wait few moments, and try again. <br>

    The Consul UI will become available once the Consul server pods are running. <br>

    ```
    kubectl get pods
    ```

    We've also included the K8s dashboard for the duration of this lab if you want to introspect the workloads using the K8s UI tab.
    We've set the dashboard up in a demo configuration, so you can use  the `skip` button to authenticate. <br>

    If you want to see a breakout of the Consul deployment from helm, run the following command. <br>

    ```
    helm status lab
    ```

    Now we can deploy some apps.
  notes:
  - type: text
    contents: |-
      Connect can be used with Kubernetes to secure pod communication with other pods and external Kubernetes services.
      The Connect sidecar running Envoy can be automatically injected into pods in your cluster, making configuration for Kubernetes automatic. <br>

      This functionality is provided by the consul-k8s project and can be automatically installed and configured using the Consul Helm chart.
      You can read more about Helm [here](https://helm.sh/).<br>

      We recommend running Consul on Kubernetes with the same general architecture as running it anywhere else.
      There are some benefits Kubernetes can provide that eases operating a Consul cluster and Connect mesh that we will explore.
      The standard production deployment guide is still an important read even if running Consul within Kubernetes. <br>

      We use a lightweight distro of K8s for instruction in this lab called [k3s](https://k3s.io/).
      Your environment consists of one single node cluster of K8s.
      We will expose services over [NodePorts](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) in our lab for simplicity.
      This includes access to the Consul API on our K8s workstation.
  - type: image
    url: https://d33wubrfki0l68.cloudfront.net/949e085caf846f7e512f420bcbd0d1a2935e27bb/4c93c/static/img/k8s-consul-simple.png
  tabs:
  - title: Helm Config
    type: code
    hostname: kubernetes
    path: /root/consul-values.yaml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: deploy-application
  id: 71ekxt1ehjg5
  type: challenge
  title: Deploy Application
  teaser: Bring our application to the world.
  assignment: |-
    Let's start by investigating our config for the Emojify app in the code editor.
    You'll notice each of our services has the following annotations.

    ```
    consul.hashicorp.com/connect-inject
    consul.hashicorp.com/connect-service-upstreams
    ```

    These annotations tell Consul that the pod should get an Envoy sidecar to route traffic,
    and listeners for the requested service on the specified upstream port.
    If you recall our helm chart, we set the following value, so we have to be explicit with our deployment files.<br>

    ```
    connectInject:
      default: false
    ```

    We can deploy our application with the following command.

    ```
    kubectl apply -f emojify/ingress.yml
    kubectl apply -f emojify/website_v1.yml
    kubectl apply -f emojify/api.yml
    kubectl apply -f emojify/cache.yml
    kubectl apply -f emojify/facebox.yml
    ```

    We have a deny by default rule in place that is disallowing all traffic.
    Let's go ahead and update our intentions so our microservices can serve traffic. <br>

    ```
    consul intention create -allow emojify-api emojify-facebox
    consul intention create -allow emojify-api emojify-cache
    consul intention create -allow emojify-ingress emojify-api
    consul intention create -allow emojify-ingress emojify-website
    ```

    Now you should be able to access the website through our Nginx ingress container!
  notes:
  - type: text
    contents: |-
      In this lab, we will deploy a more complex microservice application called [Emojify](https://github.com/emojify-app). <br>

      The application has five distinct components that provide functionality:

      * Ingress - Nginx container that gives us access to the API & Website.
      * Website - Serves static content for the Emojify website.
      * API - Provides API to machine learning backend.
      * Cache - Cache layer for API.
      * Facebox - Provides machine learning for detecting and identifies faces in photos.

      In this section we'll explore how these services can be easily connected, monitored, and scaled with Connect. <br>

      Consul Connect supports more advanced ingress controllers, such as [Datawire's Ambassador](https://www.consul.io/docs/platform/k8s/ambassador.html),
      but we will use the Nginx container in this lab to show basic functionality.
  tabs:
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/dc1/intentions
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  difficulty: basic
  timelimit: 600
- slug: emojify-some-faces
  id: ude8vm6frk3v
  type: challenge
  title: Emojify some Faces
  teaser: You get an emoji, and you get an emoji, and you get an emoji.
  assignment: |-
    We can enter a URL into our website to `emojify` the image. If you don't have an image you'd like to use try the one below of our founders.

    ```
    https://cdn.geekwire.com/wp-content/uploads/2017/10/armon-dadgar-and-mitchell-hashimoto-630x419.jpg
    ```

    Let's take a look at the service definition Consul was able to generate based on our K8s annotations.
    We've played it in the editor tab for you. You can also view it below. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- cat  /consul/connect-inject/service.hcl
    ```

    You might have remembered creating those Consul service definitions by hand.
    In K8s, all we needed was the following to generate all the necessary configurations.

    ```
    template:
      metadata:
        labels:
          app: emojify-api
        annotations:
          "consul.hashicorp.com/connect-inject": "true"
          "consul.hashicorp.com/connect-service-upstreams": "emojify-facebox:8003,emojify-cache:8005"
    ```

    Well done! Now we can move to more advanced use cases.
  notes:
  - type: text
    contents: |-
      Now that our app is working we can Emojify some faces.
      Feel free to use your colleagues, or our sample. <br>
  tabs:
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Api -  Service
    type: code
    hostname: kubernetes
    path: /tmp/api-service.hcl
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: scale-the-app
  id: r2vgop1q59ix
  type: challenge
  title: Scale the App
  teaser: Handle increased demand
  assignment: |-
    Let's revisit the emojify app specs from earlier, and scale up the number of Facebox services providing our ML capabilities. <br>

    Go in the code editor and change the number of replicas for the facebox service to `2`.

    ```
    spec:
      replicas: 2
    ```

    Now we can scale up the service.

    ```
    kubectl apply -f emojify/facebox.yml
    ```

    Now we can verify in the Consul catalog that we have two healthy services. You can also do this in the Consul UI.

    ```
    curl -s localhost:30085/v1/catalog/service/emojify-facebox | jq '[.. |."ServiceAddress"? | select(. != null)]'
    ```

    We can do the same validation for our Envoy sidecar proxy.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters | grep facebox
    ```

    Nice work! You just leveraged dynamic service discovery inside your service mesh to scale out a workload.
  notes:
  - type: text
    contents: |-
      Now that we've seen the power of Emojify, users are flocking to the application! <br>

      In this assignment we'll scale up our backend services and preserve continuity in the mesh.
  tabs:
  - title: Facebox - Config
    type: code
    hostname: kubernetes
    path: /root/emojify/facebox.yml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: get-metrics
  id: hwu25pwhbljf
  type: challenge
  title: Get Metrics
  teaser: Please observe
  assignment: |-
    You can see pods running with Grafana and Prometheus for our monitoring stack. <br>

    ```
    kubectl get pod --selector=app=prometheus
    kubectl get pod --selector=app=grafana
    ```

    We've installed them for you via their respective helm charts.

    * [Grafana](https://github.com/helm/charts/tree/master/stable/grafana)
    * [Prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)

    You can log into Grafana using the UI tab. The credentials are below:

    * username: `admin`
    * password: `check the password tab`

    We have a sample dashboard built for you that can visualize metrics for our application stack. <br>

    Follow [these instructions](https://grafana.com/docs/reference/export_import/#importing-a-dashboard) from Grafana to import your dashboard.
    You will need to paste the raw json into Grafana per the above documentation. <br>

    ```
    https://raw.githubusercontent.com/hashicorp/consul-k8s-l7-obs-guide/master/overview_dashboard.json
    ```

    We have a application that can simulate traffic for us. Let's run it now. <br>

    ```
    kubectl apply -f emojify/traffic.yml
    ```

    Wait a few moments, and check your dashboard. Metrics will be streaming in shortly. Nice work!
  notes:
  - type: text
    contents: |-
      In the last assignment we scaled up a component of our application,
      but how would we have made that determination in the first place? <br>

      In this assignment we'll look at detailed telemetry for our microservices.
      This observability is a powerful feature of Consul Connect and gives us real-time insights on our application performance. <br>

      We're spinning up a monitoring stack for you.
      Please be patient. This should take 1-2 minutes.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Grafana - UI
    type: service
    hostname: kubernetes
    port: 30030
  - title: Grafana - Password
    type: code
    hostname: kubernetes
    path: /tmp/grafana-pass.txt
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: ship-a-new-feature
  id: w95srj56cyzm
  type: challenge
  title: Ship a New Feature
  teaser: Selectively bring users to a new version of the application.
  assignment: |-
    We have a few more files in our `emojify` folder.
    The new service we introduce will allow users to optionally purchase their photograph after its emojfied. <br>

    You'll notice in our deployment spec for the `website` we used a Consul definition called `consul.hashicorp.com/service-tags`.
    This metadata will allow us send traffic  to either `v1` or `v2` of the website based on certain conditions. <br>

    Let's start by pushing Connect's L7 routing config for this service. Each of these configurations does a few things for us. <br>

    * Resolver - Creates subsets of the website service. In our example, we use the service tags.

    ```
    kind           = "service-resolver"
    name           = "emojify-website"
    default_subset = "v1"
    subsets = {
      "v1" = {
        filter = "v1 in Service.Tags"
      }
      "v2" = {
        filter = "v2 in Service.Tags"
      }
    }
    ```

    * Splitter - Contains our traffic shaping rules. We're pretty confident in our app,
    so we're going to send 90% of the traffic to the new service for instructional purposes.

    ```
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 10
        service_subset = "v1"
      },
      {
        weight         = 90
        service_subset = "v2"
      },
    ]
    ```

    * Router   - Allows us to do some version specific testing by supplying query params.

    ```
    kind = "service-router"
    name = "emojify-website"
    routes = [
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "1"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v1"
        }
      },
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "2"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v2"
        }
      }
    ]
    ```

    Now let's apply them. <br>

    ```
    consul config write emojify/resolver.hcl
    consul config write emojify/splitter.hcl
    consul config write emojify/router.hcl
    ```

    With our new routing rules in place, let's go ahead and push the new version of the website with the payment service.

    ```
    kubectl  apply -f emojify/website_v2.yml
    kubectl  apply -f emojify/payments.yml
    consul intention create -allow emojify-ingress emojify-payments
    ```

    Once your pods are healthy, we can start our testing.
    Let's check Consul for the `Website` service and look at the `ServiceTags` value.
    We should see two tagged versions of the application. <br>

    ```
    curl -s http://127.0.0.1:8500/v1/catalog/service/emojify-website | jq
    ```

    Let's use that header value we set up earlier to look at our different versions of the website config.
    Pay attention to the `PAYMENT_ENABLED` value. <br>

    ```
    curl localhost:30000/config/env.js?x-version=1
    curl localhost:30000/config/env.js?x-version=2
    ```

    Now, refresh the app a few times, and look for `EmojifyEnterprise` in the top left banner.
    This is the v2 version of our website. <br>

    Emojify another image, and you will see a new option to purchase the image.
    Fill in some dummy details and try it now. <br>

    It looks like you don't have enough funds (don't worry, this happens every time), better call your bank! <br>

    You'll notice that Envoy is now  grouping clusters by our subset.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters  | grep emojify-website
    ```

    You can also see Envoy has updated our route definitions for our debug params and traffic weights. Let's do a quick validation. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."routes"? | select(. != null)][-1]'
    ```

    Optionally, you can shift all the traffic to the v2 version of the website now that our new feature is working properly.
    You can additionally verify Consul has propagated this change to Envoy by running the above command again.

    ```
    cat <<-EOF > /root/emojify/splitter.hcl
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 0
        service_subset = "v1"
      },
      {
        weight         = 100
        service_subset = "v2"
      },
    ]
    EOF
    consul config write emojify/splitter.hcl
    ```

    Nice work! you just used traffic shaping to safely move users to a new version of your application!
  notes:
  - type: text
    contents: |-
      Now that we scaled our app, and used some metrics to determine it's stable, we can ship a new feature of our application.

      Let's use Connect's L7 routing capabilities to a/b test our application before we roll out to all our users.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: Emojify - A/B
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 900
- slug: tracing
  id: 8cnsp3ybcrlc
  type: challenge
  title: Tracing Distributed Errors
  teaser: Down the rabbit hole we go.
  assignment: |-
    We've deployed a few Jaeger services for you while setting up this lab.
    They can be seen be running the below command.
    The Jaeger UI is also exposed in the tab. We will use it shortly to visualize some traces. <br>

    ```
    kubectl get svc --selector=app=jaeger
    ```

    We will use the following Connect configuration to configure Envoy to route our trace information to our Jaeger instance.

    * [Cluster Bootstrap](https://www.consul.io/docs/connect/proxies/envoy.html#envoy_extra_static_clusters_json)
    * [Tracing Bootstrap](https://www.consul.io/docs/connect/proxies/envoy.html#envoy_tracing_json)

    ```
      {
        "connect_timeout": "3.000s",
        "dns_lookup_family": "V4_ONLY",
        "lb_policy": "ROUND_ROBIN",
        "load_assignment": {
            "cluster_name": "jaeger_9411",
            "endpoints": [
                {
                    "lb_endpoints": [
                        {
                            "endpoint": {
                                "address": {
                                    "socket_address": {
                                        "address": "jaeger-collector",
                                        "port_value": 9411,
                                        "protocol": "TCP"
                                    }
                                }
                            }
                        }
                    ]
                }
            ]
        },
        "name": "jaeger_9411",
        "type": "STRICT_DNS"
      }
    ```

    ```
      {
        "http": {
          "name": "envoy.zipkin",
          "config": {
            "collector_cluster": "jaeger_9411",
            "collector_endpoint": "/api/v1/spans",
            "shared_span_context": false
          }
        }
      }
    ```

    Let's go ahead and apply that config.

    ```
    consul config write trace.hcl
    ```

    Now let's push out the tracing application, and configure our intentions.

    ```
    kubectl apply -f tracing

    consul intention create -allow web api
    consul intention create -allow api cache
    consul intention create -allow api payments
    consul intention create -allow payments currency
    ```

    Once the pods are running we can test the application through the `Frontend`, which we've exposed over a NodePort.
    We've configured a 50% error rate for one of our backend services, so run this command twice.

    ```
    curl localhost:30900 | jq
    curl localhost:30900 | jq
    ```

    Go to the Jaeger UI and check out the traces for the last two API requests.
    For the failed API request, it is easy for us to determine where it occurred. <br>

    Trace data sent by Envoy will have `component` span tag with a value of `proxy`.
    You can see a detailed description of all the data Envoy sends [here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing#what-data-each-trace-contains). <br>

    The other spans are instrumented with the application client library for tracing.
    Compare the differences in the span tags and logs. Notice the granularity of the log events instrumented by the application's client library. <br>

    We can do a quick validation that Consul was able to apply the tracing definition to our Envoy proxy config.
    Let's check this out for our API service.

    ```
    kubectl exec $(kubectl get pod --selector=app=api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."tracing"? | select(. != null)]'
    ```

    Nice work, you just debugged a distributed app with tracing.
  notes:
  - type: text
    contents: |-
      In this assignment we will take a look at Connect's tracing capabilities. <br>

      Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures.
      It can be invaluable in understanding serialization, parallelism, and sources of latency. <br>

      Envoy helps Connect do this in a few ways:

      * Generates request IDs and trace headers for requests as they flow through the proxies
      * Sends the generated trace spans to the tracing backends
      * Forwards the trace headers to the proxied application

      We'll explore this in this challenge using popular tracing solutions Zipkin and Jaeger. <br>
  - type: text
    contents: |-
      Connect sidecars will proxy both inbound and outbound requests, but it  does not automatically know how to correlate them.
      This correlation is called `header propagation`, and requires instrumentation at the application level. <br>

      Our sample app does this with `zipkin-go` libraries and the OpenTracing API.
      The application compliments the trace data sent by the proxies with useful span tags and logs for better traceability. <br>
  - type: text
    contents: |-
      We will use a purpose built app called `fake-service` to test our upstreams and trace these errors.
      The app will deploy five services in the following configuration: <br>

      * Frontend - Access to our application
      * API - gRPC API to backend services
      * Cache - Cache responses for our API
      * Payments  - Process payments
      * Currency - Do currency lookups for our payments

      The application code can be found here: https://github.com/nicholasjackson/fake-service
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: App UI
    type: service
    hostname: kubernetes
    path: /ui
    port: 30900
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=web
    port: 31686
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 900
- slug: ingress
  id: ydjcyyarz7ef
  type: challenge
  title: Ingress
  teaser: Use advanced ingress capabilities.
  assignment: |-
    We've configured Ambassador for you in this lab with the following K8s deployment files: <br>

    * https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml
    * https://www.getambassador.io/yaml/ambassador/ambassador-service.yaml
    * https://www.getambassador.io/yaml/consul/ambassador-consul-connector.yaml

    We've also enabled [Ambassador tracing](https://www.getambassador.io/user-guide/tracing-tutorial-zipkin/), so we can see the gateway ingress in the Jaeger UI. <br>

    You can follow this [guide](https://www.getambassador.io/user-guide/consul/) to see all the integrations with Consul. <br>

    For the integration to work, we need to set up an Ambassador resolver that can query Consul's catalog. <br>

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v1
    kind: ConsulResolver
    metadata:
      name: lab-consul-dc1
    spec:
      address: lab-consul-server.default.svc.cluster.local:8500
      datacenter: dc1
    EOF
    ```

    That's it! Ambassador is ready to go. Let's create a mapping for the application. <br>

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v1
    kind: Mapping
    metadata:
      name: consul-web-mapping-tls
    spec:
      prefix: /web/
      service: web-sidecar-proxy
      resolver: lab-consul-dc1
      tls: ambassador-consul
      load_balancer:
        policy: round_robin
    EOF
    ```

    At this step, you can look at the route in the Ambassador UI tab and check its health. <br>

    We can also view all the mappings we've created, or a specific mapping.
    Alternatively, you can manage these [inline](https://www.getambassador.io/reference/mappings/#mapping-resources-and-crds). <br>

    ```
    kubectl get mappings
    kubectl describe mapping consul-web-mapping-tls
    ```

    Ambassador is also trusted in the mesh, and has it's own certificate to present to the proxies.
    The integration manages this as a K8s tls secret. We can see this below. <br>

    ```
    kubectl get secrets ambassador-consul-connect -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -text -noout
    ```

    We need to tell Connect to allow this traffic. You can do this selectively on a per service basis. <br>

    ```
    consul intention create -allow ambassador web
    ```

    Remember, our ingress controller is sitting behind the K3s load balancer. <br>

    ```
    kubectl describe svc ambassador
    ```

    The `web` service will be available at the prefix path from our mapping.
    We've reset the error rate, so all the API calls will suceed. <br>

    Let's get the load balancer IP, and send our cluster some traffic! <br>

    ```
    ambassador=$(kubectl get svc ambassador  -o json | jq -r '.status.loadBalancer.ingress[0].ip')
    curl ${ambassador}/web/
    ```

    Nice work! Log into the Jaeger UI and look at the new traces.
    You'll see additional traffic from Ambassador. <br>

    Nice work!
  notes:
  - type: text
    contents: |-
      Consul supports a rich ecosystem of integrations, and Connect is no exception.
      In this assignment we'll leverage [Datawire's Ambassador](https://www.consul.io/docs/platform/k8s/ambassador.html)
      to allow K8s ingress for our tracing application from the prior assignment. <br>

      Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster.
      Ingress can be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and more. <br>

      It can be very expensive to run dedicated load balancers per application.
      Ingress can help us manage this problem effectively. <br>

      An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer.
      We will use the [MetalLB load balancer](https://metallb.universe.tf/) to send Ambassador the traffic. <br>

      Let's try this out with Ambassador and Connect.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=web
    port: 31686
  - title: Ambassador UI
    type: service
    hostname: kubernetes
    path: /ambassador/v0/diag/
    port: 31877
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 300
checksum: "7707107130956161914"
