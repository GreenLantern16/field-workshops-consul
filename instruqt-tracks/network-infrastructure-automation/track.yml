slug: raj-nia-ga
id: xpsqf8txwytb
version: 0.0.1
type: track
title: Network Infrastructure Automation - Update
teaser: Automate the provisioning and ongoing management of network infrastructure.
description: In this workshop you'll learn how to automate the provisioning and management
  of traditional network infrastructure using Terraform, Consul, and Consul Terraform
  Sync. Eliminate exposure to operational overhead and security risk while accelerating
  the deployment and scaling of applications and services.
icon: https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/NIA-CTS-Icon.png
tags:
- consul
- NIA
owner: hashicorp
developers:
- lance@hashicorp.com
- npearce@hashicorp.com
- raj@hashicorp.com
private: true
published: true
show_timer: true
challenges:
- slug: 1-review-lab-objectives
  id: yxebl6snpd3o
  type: challenge
  title: Review Lab Objectives
  teaser: Before we begin, lets quickly review the architecture.
  assignment: |
    "In this workshop, we will build the infrastructure depicted in the 'NIA - Traffic Flow' tab on the left, but before we do, let's quickly review the operational challenges we are going to address.

    ## Navigate to the "NetOps - Operational Pattern" tab.

    In this diagram we have a two-tier application running on virtual machines that use Consul for Service Discovery. New virtual machines are frequently added and removed to handle scaling requirements. The overworked NetOps and SecOps teams have to frequently reconfigure the load balancers and firewalls.

    The long hours and lengthy job queues result in deployment errors and, potentially security issues.

    ## Navigate to the "NIA - Operational Pattern" tab.

    `Consul` monitors application state changes in real-time (IP Addresses and App meta-data). `Consul Terraform Sync` uses this information to automatically configure the various network infrastructure, eliminating the need for NetOps teams to be involved after the initial configuration.

    ## Navigate to the "NIA - Traffic Flow" tab.

    In this diagram you can see the Traffic Flow of the infrastructure we are going to build in this workshop.

    NOTE: Consul Terraform Sync automates the management of policy address groups on the Palo Alto Firewalls. This facilitates having fine-grained policies without increasing operational overhead.

    In the next few challenges we are going to build this cloud environment, enabling you to see this workflow in operation."
  notes:
  - type: text
    contents: |-
      Every journey begins with a single step!

      Step 1, review the environment you will build using Terraform, and manage using Consul Terraform Sync.
  tabs:
  - title: NetOps - Operational Pattern
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/1.NIA-Workshop-NetOps.html
  - title: NIA - Operational Pattern
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/1.NIA-Workshop-CTS.html
  - title: NIA - Traffic Flow
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/1.NIA-Workshop-CTS_Traffic.html
  difficulty: basic
  timelimit: 300
- slug: 2-provision-azure-vnets
  id: cnozq7gjfmci
  type: challenge
  title: Provision Azure VNETs
  teaser: Deploy basic network infrastructure using Terraform
  assignment: |-
    In this assignment you will provision the VNets in the "Current Lab Setup" tab that will used in the following assignments.

    The "Terraform Code" tab contains all of the Infrastructure as Code (IaC) templates used by Terraform to build the VNETs. Feel free to look over the code!

    In the `Shell` tab, deploy the Azure VNETs by running the following commands:
    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Their CIDR blocks are listed below:
    ```
    hcs-vnet: 10.0.0.0/16
    shared-svcs-vnet: 10.2.0.0/16
    app-vnet: 10.3.0.0/16
    ```

    The subnets created within 'app-vnet' are as follows:
    ```
    MGMT: 10.3.1.0/24
    INTERNET: 10.3.2.0/24
    DMZ: 10.3.3.0/24
    APP: 10.3.4.0/24
    ```

    You will leverage these VNETs in the next few assignments.
  notes:
  - type: text
    contents: |
      Setting up your environment... Your Azure account will be ready in ~5 minutes.
      Keep an eye on the bottom right corner to know when you can get started.
  tabs:
  - title: Current Lab Setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/2.NIA-Workshop-VNETs.html
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/vnet
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 3000
- slug: 3-provision-core-services
  id: tldzdwktbfgy
  type: challenge
  title: Provision Core Services
  teaser: Provision Vault and HCS using Terraform
  assignment: |
    With the VNETs provisioned, you are now ready to deploy HCS (Hashicorp Consul Service, aka. Consul-as-a-Service), Vault, and a Bastion Host used to access various services in future challenges.

    Terraform will be used to provision each resource.

    ## HashiCorp Vault
    Vault is a secrets management solution that we will use to securely store sensitive information such as usernames, passwords, certificates, and tokens.

    You can review the Terraform templates used to deploy Vault, in the 'Vault Terraform Code' tab.

    When ready, switch to the `Shell` tab and run the following commands:
    ```
     cd /root/terraform/vault
     terraform plan
     terraform apply -auto-approve
    ```

    ## HCS
    Let's provision HCS.<br>
    In the `Shell` tab run the following commands.

     ```
     cd /root/terraform/hcs
     terraform plan
     nohup terraform apply -auto-approve > /root/terraform/hcs/terraform.out &
     ```

    NOTE: HCS will continue to provision in the background while we move on to the next challenge.
  notes:
  - type: text
    contents: |
      Terraform allows you to document, share, and deploy environments in one workflow by using Infrastructure as Code!
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/3.NIA-Workshop-Core_Svcs.html
  - title: Vault Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/vault
  - title: Consul Server Code
    type: code
    hostname: workstation
    path: /root/terraform/consul-server
  - title: HCS Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/hcs
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 3000
- slug: 4-provision-network-infra
  id: zckpjujeiczv
  type: challenge
  title: Provision F5 BIG-IP & Palo Alto Firewall
  teaser: Provision an F5 BIG-IP VE & Palo Alto Firewall using Terraform
  assignment: |
    First we will provision the Palo Alto Firewall.

    In the `Shell` tab run the following commands.
    ```
    cd /root/terraform/panw-vm
    terraform plan
    nohup terraform apply -auto-approve > /root/terraform/panw-vm/terraform.out &
    ```

    NOTE: The Palo Alto Firewall will continue to provision in the background allowing you to continue to the next step.

    Now we will provision the F5 BIG-IP Virtual Edition using Terraform.

    In the `Shell` tab run the following commands.
    ```
    cd /root/terraform/bigip
    terraform plan
    terraform apply -auto-approve
    ```

    This can take several minutes to complete. While you wait, feel free to look over the `Palo Alto Terraform Code` and `F5 BIG-IP Terraform Code` tabs to see review the Terraform code.

    Once the Terraform apply is complete, the BIG-IP is accessible using the IP address provided in the Terraform output.

    **NOTE:** You will need to open the BIG-IP URL in a separate browser tab. If you are using chrome, you may be presented with a certificate error. To bypass this error, type "thisisunsafe" into the Chrome window.

    The HCS services may not be available quite yet, but is required before we can move on to the next challenge. You can monitor its progress with the following command:

    Monitor HCS provisioning
    ```
    tail -f /root/terraform/hcs/terraform.out
    ```
  notes:
  - type: text
    contents: |
      In this exercise we will be provisioning an F5 BIG-IP Virtual Edition and Palo Alto VM Series Firewall using Terraform.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/4.NIA-Workshop-F5_PA.html
  - title: Palo Alto Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/panw-vm
  - title: F5 BIG-IP Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/bigip
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 3000
- slug: 5-configure-palo-alto-firewall
  id: mqhktdfvrx5f
  type: challenge
  title: Configure Palo Alto Firewall
  teaser: Provision a Palo Alto VM-Series Firewall using Terraform.
  assignment: |-
    Now we will configure the Palo Alto Firewall using Terraform.

     First, verify that the Palo Alto VM firewall setup has completed.
     In the `Shell` tab run the following command:

     ```
    tail /root/terraform/panw-vm/terraform.out -n 12
    ```

     You should see `Apply complete!`. If it hasn't finished yet, review the Terraform code that will be used to configure the firewall, in the 'Terraform Code' tab.

    When ready, configure the firewall:

     ```
    terraform plan
    terraform apply -auto-approve
    ```

    Now we must instruct the firewall to `commit` this change using the following command:
    ```
    /root/terraform/panos_commit/panos-commit -config /root/terraform/panos_commit/panos-commit.json -force
    ```

    Once the apply has completed, Open up new browser tabs to access the Palo Alto and BIG-IP Virtual Machines using the details in the 'Access Info' tab.

    For CLI access to the Firewall execute:
    ```
    ssh -q -A -J azure-user@$bastion_ip paloalto@$firewall_ip
    ```
  notes:
  - type: text
    contents: Next we shall configure the newly deployed Palo Alto virtual machine.
  tabs:
  - title: Provision PANW
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/4.NIA-Workshop-F5_PA.html
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/panw-config
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Access Info
    type: code
    hostname: workstation
    path: /access.md
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 300
- slug: 6-validate-hcs
  id: lpr5ynqurocm
  type: challenge
  title: Validate HCS
  teaser: Verify Vault, HCS, and Consul are operational
  assignment: |
    Consul HCS and Vault should now be provisioned and accessible from the corresponding tabs.

    In this exercise we will gather the information required to connect to HCS and securely store this information in Vault.

    In the `Shell` tab run the following commands.
    ```
    vault login -method=userpass username=operations password=Password1
    ```

    Retrieve the bootstrap token and gossip key from HCS and save it to your Vault instance:

    ```
    echo $CONSUL_HTTP_ADDR
    echo $VAULT_ADDR
    bootstrap_token=$(az hcs create-token --resource-group $(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name) --name hcs | jq  -r .masterToken.secretId)
    gossip_key=$(az resource show --ids "/subscriptions/$(az account show | jq -r .id)/resourceGroups/$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)/providers/Microsoft.Solutions/applications/hcs/customconsulClusters/hcs" --api-version 2018-09-01-preview | jq -r .properties.consulConfigFile | base64 -d | jq -r .encrypt)
    vault kv put secret/consul master_token=${bootstrap_token} gossip_key=${gossip_key}
    ```

    Now inspect the credentials:

    ```
    echo $VAULT_ADDR
    vault kv get secret/consul
    ```
    Operators can use this master token to login and explore the Consul UI. However, master token usage should be highly restricted. So instead, let's configure Vault to [dynamically ](https://www.vaultproject.io/docs/secrets/consul/) issue Consul tokens for Operators, so we don't share or over-use the Consul master token.<br>

    First, let's generate a new Consul token for Vault to use whenever Vault dynamically generates Consul Operator tokens in the future:
    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field=master_token secret/consul)
    vault_consul_mgmt_token=$(consul acl token create -policy-name=global-management -description "vault mgmt" | grep SecretID | cut -d ":" -f2 | xargs)

    ```
    Now configure the Vault secrets engine that dynamically creates additional Consul operator tokens:

    ```
    vault write consul/config/access address=${CONSUL_HTTP_ADDR} token=${vault_consul_mgmt_token}
    vault read consul/config/access
    ```

    Finally, create a Consul policy for the Operations team and link it to the Vault Operations role:

    ```
    consul acl policy create -name "ops" -description "admin policy for ops" -rules 'acl = "write" operator = "write" namespace_prefix "" {acl = "write"}'
    vault write consul/roles/ops policies=ops ttl=1h
    ```

    Use Vault to dynamically generate a new Consul Operator token:
    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Our new Consul Operator token is used to set up a policy called "anonymous".
    ```
    echo '
    node_prefix "" {
      policy = "read"
    }
    service_prefix "" {
      policy = "read"
    }
    session_prefix "" {
      policy = "read"
    }
    agent_prefix "" {
      policy = "read"
    }
    query_prefix "" {
      policy = "read"
    }
    operator = "read"' | consul acl policy create -name anonymous -rules -
    consul acl token update -id anonymous -policy-name anonymous
    ```

    You will use this "anonymous" role in a later assignment to configure access for Consul service consumers.
  notes:
  - type: text
    contents: |
      Next we shall verify the core services are operating as expected and then configure HCS to use Vault secrets for its tokens.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/6.NIA-Workshop-Tokens.html
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 3000
- slug: 7-deploy-app-environments
  id: cqxtosyyagvf
  type: challenge
  title: Deploy App environments
  teaser: Deploy a 2-tier VM based application to the cloud.
  assignment: |-
    In this assignment we will be deploying the application into Azure based VMs.

    The application uses Consul for Service Discovery. The VMs are configured to run a Consul agent that automatically registers these services into Consul.

    Before Consul was implemented, the application relied upon static IP addresses which were hardcoded into the configuration and application code. Now, IP addresses no longer have to be known before the applications can be provisioned. Thus decoupling steps in the provisioning workflow.

    Review the code in the `Terraform Code` tab. This defines the VMSS (virtual machine scaling) for the web and app tiers of the application.

    Begin provisioning the application in the background:

    ```
    terraform plan
    nohup terraform apply -auto-approve > deploy_app.log 2>&1 &
    ```

    By registering nodes and services in Consul, other services can easily discover their health status and network location.

    Since the new App and Web services auto-register themselves into Consul, you can now watch the progress of the App deployment by watching the `Consul` tab.

    You could also watch the output of the app deployment log by executing the following command in the `Shell` tab:
    ```
    tail -f /root/terraform/app/deploy_app.log
    ```
    You will explore the environment in more detail in the next challenge.
  notes:
  - type: text
    contents: |
      Now we'll deploy the last piece of infrastructure before experiencing the magic of Network Infrastructure Automation w/ Consul Terraform Sync.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/7.NIA-Workshop-App_Deploy.html
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/app
  - title: Access Info
    type: code
    hostname: workstation
    path: /access.md
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Shell
    type: terminal
    hostname: workstation
  difficulty: basic
  timelimit: 3000
- slug: 8-install-consul-terraform-sync
  id: uksggixqc3yy
  type: challenge
  title: Deploy 'Consul Terraform Sync'
  teaser: Now we are going install Consul Terraform Sync and bring dynamic service
    discovery to network infrastructure.
  assignment: |-
    Before installing Consul Terraform Sync, lets take a look at the F5 BIG-IP and the Palo Alto Firewall.

    Using the credentials in the `Access Info` tab, open a new browser window and login to the BIG-IP and Palo Alto Firewall.

    ## BIG-IP Configuration

    In the BIG-IP UI, click on 'Local Traffic' > 'Network Map' (a new window will open). In this new window, change the partition from 'Common' to 'All' and then reload the page with `Cmd/Ctrl+R`.

    This will be empty. Typically a BIG-IP administrator would now configure the Virtual IP, all the protocol profiles, the monitor, the load-balancer pool, and then add all the web servers to the pool. Consul Terraform Sync is going to take care of this for us.

    ## Palo Alto Configuration

    Switch to the Palo Alto browser tab. Under the 'Policies' tab, note that the second rule "Allow traffic from BIG-IP to App" has a Destination Address Group (DAG) titled "cts-add-grp-web".

    Navigate to the "Objects" tab and select "Address Groups". Under the 'Addresses' column for "cts-add-grp-web" click on "more...". Note that the Address Group is empty. Consul Terraform Sync is going to take care of this.

    Lastly, in the Lab Console navigate to the 'App' tab and note that the App is not currently available (502 Error).

    ## Deploy Consul Terraform Sync

    Now it's time for Consul Terraform Sync to configure all the Network Infrastructure operations.
    ```
    cd /root/terraform/consul-tf-sync/
    terraform plan
    terraform apply -auto-approve
    ```

    When completed, connect to the CTS image:
    ```
    consul_terraform_sync=$(curl -s $CONSUL_HTTP_ADDR/v1/catalog/node/consul-terraform-sync | jq -r '.Node.Address')
    ssh -q -A -J azure-user@$bastion_ip azure-user@$consul_terraform_sync
    ```

    Take a look at the `consul-terraform-sync` configuration:
    ```
    cat /etc/consul-tf-sync.d/consul-tf-sync.hcl
    ```

    Review the terraform-consul-sync service status with the following command
    ```
    service consul-tf-sync status
    ```

    Look over the Palo Alto & BIG-IP configurations again, per the steps above. Note that their configurations have been updated!

    Next, navigate to the `App` tab. The application is now available through the Firewall and is Load-balanced.

    Lastly, navigate back to the 'Current Lab Setup' tab and marvel at the beauty of Network Infrastructure Automation. You did this!

    Move to the next assignment when you are ready.
  notes:
  - type: text
    contents: |
      Now we're going to eliminate a lot of operational friction by installing `consul-terraform-sync`. This will auto-update network infrastructure as service changes occur!
  tabs:
  - title: Current Lab Setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/8.NIA-Workshop-CTS_Install.html
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/consul-tf-sync
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Access Info
    type: code
    hostname: workstation
    path: /access.md
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: App
    type: service
    hostname: workstation
    path: /ui
    port: 8080
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 3000
- slug: 9-scale-the-application
  id: pexgsho8syxz
  type: challenge
  title: Scale the application
  teaser: Let Consul take care of routine adds/moves/changes of services instances.
  assignment: |-
    "Now let's re-run the Terraform code for the application, with some new variables to scale the application out:

    ```
    terraform apply -var app_count=3 -var web_count=3 -auto-approve
    ```

    After the Terraform run completes, you can monitor the status of your nodes and services using the Consul UI. Once all of the new instances are online and healthy, you can revisit some of the things we reviewed in the previous exercises.

    View the nodes and services via CLI:

    ```
    consul members
    consul catalog services
    ```

    Review all of the web instances:
    ```
    curl -s $CONSUL_HTTP_ADDR/v1/catalog/service/web | jq
    ```

    Review all of the app instances:
    ```
    curl -s $CONSUL_HTTP_ADDR/v1/catalog/service/app | jq
    ```

    Once the new instances have finished booting, review the Palo Alto address group and the BIG-IP Pool member list once again and note that they now have three addresses to
    reflect the increased server scale group.

    The resources for this lab will self-destruct in 8 hours, but to save a little money, **please scale the application back down.**

    Re-run Terraform, and monitor the various integration points once again. We'll do so in the background so that you can move on whenever you're ready.

    ```
    nohup terraform apply -var app_count=1 -var web_count=1 -auto-approve > /root/terraform/app/scaledown.out &
    ```

    This concludes the final step in the Network Infrastructure Automation workshop.
  notes:
  - type: text
    contents: |
      75% companies surveyed take Days or even Weeks to complete networking tasks.

      Organizations seeking to improve application delivery cycle are often blocked at the networking layer

      [source](https://zkresearch.com/research/2017-application-delivery-controller-study)
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/network-infrastructure-automation/assets/images/9.NIA-Workshop-App_Scale.html
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/app
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Access Info
    type: code
    hostname: workstation
    path: /access.md
  - title: App
    type: service
    hostname: workstation
    path: /ui
    port: 8080
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  difficulty: basic
  timelimit: 3000
checksum: "3837996945260429650"
