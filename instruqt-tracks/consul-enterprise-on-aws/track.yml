slug: consul-enterprise-on-aws
id: stkflr7xh9ud
type: track
title: Consul Enterprise on AWS
teaser: A short description of the track.
description: THIS TRACK IS UNDER ACTIVE DEVELOPMENT!!! Things may periodically break!!
icon: https://storage.googleapis.com/instruqt-frontend/img/tracks/default.png
tags: []
owner: hashicorp
developers:
- lance@hashicorp.com
private: true
published: true
challenges:
- slug: provision-vpcs
  id: ujbryneafql8
  type: challenge
  title: 'Infrastructure: Provision VPCs'
  teaser: A short description of the challenge.
  assignment: |-
    The cloud infrastructure team is responsible for providing VPCs to shared service and development teams.
    You can think of workstation terminal as your laptop machine preloaded with cloud access credentials for your company. <br>

    Any any time you can use the AWS console to view your environment.
    AWS CLI commands will be provided for you to interact with AWS's APIs. <br>


    Inspect the Terraform code, and then provision the cloud environment.

    ```
    terraform apply
    ```

    You'll notice you have four separate VPCs that map to the infra diagram shared earlier.

    ```
    aws ec2 describe-vpcs
    ```

    Their CIDR blocks are listed below:

    ```
    terraform-vpc-shared-svcs: 10.1.0.0/16
    terraform-vpc-frontend: 10.2.0.0/16
    terraform-vpc-api: 10.3.0.0/16
    terraform-vpc-storage: 10.4.0.0/16
    ```

    Our shared service VPC will run the core Consul infrastructure.
    You will provision the Consul servers in the next assignment.
  notes:
  - type: text
    contents: |-
      In this assignment you will familiarize yourself with the AWS cloud
      environment. <br>

      Please be patient as we provision your cloud environment in AWS.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vpc
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 18000
- slug: build-consul-image
  id: x3xlgqyvidby
  type: challenge
  title: 'Operations: Build Consul Image'
  teaser: A short description of the challenge.
  assignment: |-
    In this assignment you will build an immutable image of [Consul with HashiCorp Packer](https://packer.io/) <br>

    Immutability has many advantages for infrastructure management.
    Consul enterprise can take advantage of immutable patterns with [Automated Upgrades](https://www.consul.io/docs/enterprise/upgrades/index.html)

    The Packer build has been set up for you. Inspect the variable file, then build the image.

    ```
    cat /root/packer/vars.json
    AWS_REGION=us-east-1 packer build -var-file vars.json centos.json
    ```

    Validate your AMI is available in us-east-1.

    ```
    aws ec2 describe-images --owners self
    ```

    Now that you have a Consul image, you're ready to provision Consul in a prod-like ASG.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Packer
    type: code
    hostname: cloud-client
    path: /root/packer
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 18000
- slug: provision-consul
  id: jhvf5b2rlhmk
  type: challenge
  title: 'Operations: Provision Consul'
  teaser: A short description of the challenge.
  assignment: |-
    In this assignment, you'll install the ASG that will run the Consul servers.
    The terraform module is configured to use the packer image you created in the last assignment. <br>

    Inspect the variable file, then deploy Consul.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform apply
    ```

    Now, validate the Consul servers deployed to AWS. <br>

    ```
    aws autoscaling describe-auto-scaling-groups
    ```

    You should see the Consul servers with a ``*-0.0.1` postfix.
    You will increment the deployment in the next exercise to finish the immutable bootstrap.

    Let's inspect one of the newly provisioned Consul instances.
    We can access the newly provisioned instance through our bastion host helper script.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    Go to the next assignment, and finish the bootstrap.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: bootstrap-consul
  id: 7mefmboypk7h
  type: challenge
  title: 'Operations: Bootstrap Consul'
  teaser: A short description of the challenge.
  assignment: |-
    Consul Enterprise supports an [upgrade pattern](https://learn.hashicorp.com/consul/day-2-operations/autopilot#upgrade-migrations) that allows operators to deploy a complete cluster of new servers,
    and safely migrate the new servers until the upgrade is complete. <br>

    In one of the terminal window, you will watch the new servers added as `non-voters`.
    Once autopilot has validated the new servers, it will safely promote them.
    You will see the old servers removed from the list.

    ```
    watch consul operator raft list-peers
    ```

    Next, validate the following two changes to the `consul.auto.tfvars file` to complete the bootstrap:
      * consul_cluster_version = "0.0.2"
      * bootstrap = false

    Now run terraform, and watch the servers automatically transition.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform apply
    ```

    Nice work! You just did an automated server update with Consul enterprise. <br>

    You can now validate the changes. Notice the `node_meta` is for the `0.0.2` version of the deployment. <br>

    ```
    consul catalog nodes --detailed
    ```

    You can also see that the `default_policy` for ACLs was updated for the new value.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    Now you can start setting up the tenants for the Consul shared service consumers.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: centralize-consul-secrets
  id: 8whoxkkzchx0
  type: challenge
  title: 'Security: Centralize Consul Secrets'
  teaser: A short description of the challenge.
  assignment: |-
    Terraform and Vault are commonly used together to provide secure workflows for credentials.
    Terraform Enterprise provides additional features on top of Terraform to make this easier. <br>

    The bootstrap outputs have been secured in Vault.
    You will access these later to configure additional Consul roles for humans and machines. <br>

    Login as an operator and inspect the credentials.  <br>

    ```
    vault login -method=userpass username=operations password=Password1
    ```

    Now inspect the credentials.

    ```
    vault kv get secret/consul
    ```

    You can use the master token to create a management token for Vault to issue [dynamic secrets](https://www.vaultproject.io/docs/secrets/consul/) for Consul.
    Dynamic secrets have many advantages. <br>

    Get a management token for Vault to manage Consul tokens with.
    You can retrieve the privileged token for this operation from Vault.  <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field=master_token secret/consul)
    vault_consul_mgmt_token=$(consul acl token create -policy-name=global-management -description "vault mgmt" | grep SecretID | cut -d ":" -f2 | xargs)
    echo $vault_consul_mgmt_token
    ```

    Now configure the secrets engine.

    ```
    vault write consul/config/access address=${CONSUL_HTTP_ADDR} token=${vault_consul_mgmt_token}
    vault read consul/config/access
    ```

    Last, create a policy for the operations team, and link it to the Vault role.
    We will put a short ttl of one hour on these highly privileged administrative tokens.

    ```
    consul acl policy create -name "ops" -description "admin policy for ops" -rules 'acl = "write" operator = "write" namespace_prefix "" {acl = "write"}'
    vault write consul/roles/ops policies=ops ttl=1h
    ```

    Now you are ready to get a dynamic Consul token from Vault.

    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Nice work. You'll leverage this dynamic role in the next exercise.
  notes:
  - type: text
    contents: A Vault instance is spinning up for you. Please be patience.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  difficulty: basic
  timelimit: 300
- slug: create-namespaces-and-policies
  id: g7hylnrby0x9
  type: challenge
  title: 'Operations: Create Namespaces and Policies'
  teaser: A short description of the challenge.
  assignment: |-
    Consul enterprise support [namespaces](https://www.consul.io/docs/enterprise/namespaces/index.html) to enable network and operations teams to provide self-service for development teams. <br>

    Fetch a dynamic operater token from vault.
    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Start by creating the shared service discovery policy that will underpin the development team tenants.
    This will allow the mesh to discover services across teams, but teams will control the services that can connect to services they run.
    We will also create a policy for the K8s injector. <br>

    ```
    consul acl policy create -name "cross-namespace-policy" -description "cross-namespace service discovery" -rules @cross-namespace.hcl
    consul acl policy create -name "k8s-injector-policy" -description "k8s injection" -rules @injector.hcl
    ```

    Now you can create the namespaces for each development team, that share the namespace discovery policy. <br>

    ```
    consul namespace write frontend-namespace.hcl
    consul namespace write api-namespace.hcl
    ```

    You also need to create policies for the nodes that each team will run in their VPC.
    We will restrict this by CIDR block. <br>

    ```
    consul acl policy create -name "frontend-agent-policy" -description "frontend agents" -rules @frontend-team-agent.hcl
    consul acl policy create -name "api-agent-policy" -description "api agents" -rules @api-team-agent.hcl
    ```

    Last, create policies for the development teams to manage their own intentions.
    Intentions allow connectivy between services. <br>

    ```
    consul acl policy create -name "frontend-developer-policy" -description "frontend dev" -rules @frontend-team-developer.hcl
    consul acl policy create -name "api-developer-policy" -description "api dev" -rules @api-team-developer.hcl

    ```

    Nice work! In the next assigment we will link these policies to Vault roles.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Policies
    type: code
    hostname: cloud-client
    path: /root/policies
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  difficulty: basic
  timelimit: 1800
- slug: create-consul-roles
  id: r0oxqx8ait5u
  type: challenge
  title: 'Security: Create Consul Roles'
  teaser: A short description of the challenge.
  assignment: |-
    Link the Consul policies to Vault roles so the various users in the organization can request Consul ACL tokens.
    Get an operator token for Consul. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    We will create scoped TTLs for each role.
    The agent tokens will be rotated periodically, and will have a longer TTL.

    ```
    vault write consul/roles/k8s-injector       policies=k8s-injector-policy       ttl=720h
    vault write consul/roles/frontend-agent     policies=frontend-agent-policy     ttl=720h
    vault write consul/roles/frontend-developer policies=frontend-developer-policy ttl=1h
    vault write consul/roles/api-agent          policies=api-agent-policy          ttl=720h
    vault write consul/roles/api-developer      policies=api-developer-policy      ttl=1h
    ```

    You will use these roles and policies to configure the Kubernetes clusters and onboard the development teams in a later assignment.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 300
- slug: create-vpc-peering
  id: suzx6ufcymhe
  type: challenge
  title: 'Network: Create Transitive Peering'
  teaser: A short description of the challenge.
  assignment: |-
    The network team is responsible for creating connectivity between VPCs, and on-prem environments.
    On AWS the network team leverages [AWS Transit Gateway](https://aws.amazon.com/transit-gateway/) to reduce management overhead. <br>

    Run Terraform to setup the transit gateway routes. <br>

    ```
    terraform apply
    ```

    Now inspect one of the route tables. Let's look at the route table for the API team private subnets.

    ```
    route_table=$(/usr/local/bin/terraform output -state /root/terraform/vpc/terraform.tfstate -json api_private_route_table_ids | jq -r .[0])
    aws ec2 describe-route-tables --route-table-ids ${route_table}
    ```

    You'll notice routes to CIDR blocks for other application teams route through the transit gateway.  <br>

    You will expand the transit gateway to connect to the on-prem enviroment in a later assignment.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/tgw
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: provision-eks-clusters
  id: oxwep1bahgcl
  type: challenge
  title: 'Infrastructure: Provision EKS clusters'
  teaser: A short description of the challenge.
  assignment: |-
    The API and Frontend teams run their applications on AWS EKS.
    The infrastructure team is responsible for provisioning EKS clusters. <br>


    Inspect the Terraform for EKS, then provision the Kubernetes clusters.
    EKS can take over ~10 minutes to provision, so you will run this as a background task. <br>

    ```
    nohup terraform apply -auto-approve > terraform.out &
    tail -f terraform.out
    ```

    Validate your clusters once they are provisioned.
    The status should be `ACTIVE`. <br>

    ```
    aws eks describe-cluster --name frontend
    aws eks describe-cluster --name api
    ```

    Great work! In this next assignments we will connect these Kubernetes nodes to the Consul shared service.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/eks
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: configure-eks-cluster-segments
  id: xrckckfdqgj9
  type: challenge
  title: 'Operations: Configure EKS cluster segments'
  teaser: A short description of the challenge.
  assignment: |-
    To adhere to InfoSec policies, Consul gossip traffic needs to be restricted to App VPCs <> Shared Service VPC.
    Consul enterprise [network segments](https://www.consul.io/docs/enterprise/network-segments/index.html) support this hub-and-spoke topology.

    The network segment config has been added to the Terraform file with a new cluster version.
    Inspect it, then apply the configuration. <br>

    ```
    cat terraform.tfvars
    cat main.tf
    terraform apply
    ```

    Validate your network segments were applied to the Consul servers.

    ```
    check-consul-config /etc/consul.d/zz-override.json | jq
    ```

    Now you can configure these EKS nodes to Connect to the Consul service.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: configure-eks-cluster-secrets
  id: ld9gbmuj5jnr
  type: challenge
  title: 'Operations: Configure EKS cluster secrets'
  teaser: A short description of the challenge.
  assignment: |-
    You need to seed the Consul clusters with secrets for the client agents to join with the server nodes.
    In production, a CI/CD pipeline, Terraform, or configuration management tool can perform these tasks with Vault.
    Platforms for these systems have well defined patterns for establishing trust with Vault. <br>

    You have been provided a sample script to the following:

    * Create a namespace to run Consul pods (injector webhooks, etc.)
    * Provide a gossip key
    * Provide an ACL

    First, get an operations token from Vault.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Next, inspect the setup script.

    ```
    cat /usr/local/bin/setup-k8s-consul-cluster
    ```

    Start with the frontend cluster, and run the setup script with the Vault provided Consul secrets.
    Verify the secrets were provisioned.

    ```
    kubectl config use-context eks_frontend
    setup-k8s-consul-cluster $(vault kv get -field gossip_key secret/consul) $(vault read -field token consul/creds/frontend-agent) $(vault read -field token consul/creds/k8s-injector)
    kubectl  get secrets -n consul
    ```

    Repeat these steps on the API cluster.

    ```
    kubectl config use-context eks_api
    setup-k8s-consul-cluster $(vault kv get -field gossip_key secret/consul) $(vault read -field token consul/creds/api-agent) $(vault read -field token consul/creds/k8s-injector)
    kubectl get secrets -n consul
    ```

    Now you can provision the Consul client agents in the EKS clusters.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: deploy-consul-in-eks
  id: 3ewjpnxsjsmh
  type: challenge
  title: 'Operations: Deploy Consul in EKS'
  teaser: A short description of the challenge.
  assignment: |-
    The Consul helm chart will allow us to easily deploy the agents into the cluster.
    The ACL init job will use the credentials you previously provisioned.

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    helm install hashicorp -f frontend-values.yaml --namespace consul ./consul-helm --wait
    kubectl get pods -n consul
    ```

    Next deploy the API cluster.

    ```
    kubectl config use-context eks_api
    helm install hashicorp -f api-values.yaml --namespace consul ./consul-helm --wait
    kubectl get pods -n consul
    ```

    Check the nodes for the full working set.  You will see a few members in the `left` state.
    This is expected, as they are left over from the auto upgrade and no longer in the peer set. <br>

    ```
    consul members
    ```

    Nice work! Now you need to create trust between K8s API and the Consul API.s
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 300
- slug: security-configure-k8s-consul-authentication
  id: tnkxgigifkbu
  type: challenge
  title: 'Security: Configure K8s Consul Authentication'
  teaser: A short description of the challenge.
  assignment: |-
    In this assignment you will use a [Consul auth method](https://www.consul.io/docs/acl/auth-methods/kubernetes.html)
    to securely introduce Consul ACL tokens to K8s pods. <br>

    Start by retrieving a Consul operations token from Vault. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Now you can create the auth method in the frontend namespace.
    The cross namespace policy you created earlier will be attached to the token.
    You need three key pieces of information to complete this task. <br>

    * K8s endpoint
    * K8s CA
    * Service Account JWT

    The helper script will assist you in this task.
    Review it now. <br>

    ```
    cat /usr/local/bin/setup-k8s-consul-auth
    ```

    Configure the clusters.
    ```
    setup-k8s-consul-auth frontend
    setup-k8s-consul-auth api
    ```

    You can use any GitHub flavoured markdown.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  difficulty: basic
  timelimit: 300
- slug: developer-deploy-applications
  id: dslh2b4hvc4t
  type: challenge
  title: 'Developer: Deploy Applications'
  teaser: A short description of the challenge.
  assignment: |-
    Now you can start deploying applications to the k8s clusters.

    Start with the Frontend cluster.

    ```
    kubectl config use-context eks_frontend
    kubectl apply -f frontend
    ```

    Now deploy the API cluster.

    ```
    kubectl config use-context eks_api
    kubectl apply -f api
    ```

    All the pods are ready. In the next assignment we will update intentions so traffic will route.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 300
checksum: "2635165296758465269"
