slug: consul-enterprise-on-aws
id: stkflr7xh9ud
type: track
title: Consul Enterprise on AWS
teaser: A short description of the track.
description: THIS TRACK IS UNDER ACTIVE DEVELOPMENT!!! Things may periodically break!!
icon: https://storage.googleapis.com/instruqt-frontend/img/tracks/default.png
tags: []
owner: hashicorp
developers:
- lance@hashicorp.com
private: true
published: true
challenges:
- slug: provision-vpcs
  id: ujbryneafql8
  type: challenge
  title: 'Infrastructure: Provision VPCs'
  teaser: A short description of the challenge.
  assignment: |-
    The cloud infrastructure team is responsible for providing VPCs to shared service and development teams.
    You can think of workstation terminal as your laptop machine preloaded with cloud access credentials for your company. <br>

    Any any time you can use the AWS console to view your environment.
    AWS CLI commands will be provided for you to interact with AWS's APIs. <br>


    Inspect the Terraform code, and then provision the cloud environment.

    ```
    terraform apply
    ```

    You'll notice you have four separate VPCs that map to the infra diagram shared earlier.

    ```
    aws ec2 describe-vpcs
    ```

    Their CIDR blocks are listed below:

    ```
    terraform-vpc-shared-svcs: 10.1.0.0/16
    terraform-vpc-frontend: 10.2.0.0/16
    terraform-vpc-api: 10.3.0.0/16
    terraform-vpc-storage: 10.4.0.0/16
    ```

    Our shared service VPC will run the core Consul infrastructure.
    You will provision the Consul servers in the next assignment.
  notes:
  - type: text
    contents: |-
      In this assignment you will familiarize yourself with the AWS cloud
      environment. <br>

      Please be patient as we provision your cloud environment in AWS.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vpc
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 18000
- slug: build-consul-image
  id: x3xlgqyvidby
  type: challenge
  title: 'Operations: Build Consul Image'
  teaser: A short description of the challenge.
  assignment: |-
    In this assignment you will build an immutable image of [Consul with HashiCorp Packer](https://packer.io/) <br>

    Immutability has many advantages for infrastructure management.
    Consul enterprise can take advantage of immutable patterns with [Automated Upgrades](https://www.consul.io/docs/enterprise/upgrades/index.html)

    The Packer build has been set up for you. Inspect the variable file, then build the image.

    ```
    cat /root/packer/vars.json
    AWS_REGION=us-east-1 packer build -var-file vars.json centos.json
    ```

    Validate your AMI is available in us-east-1.

    ```
    aws ec2 describe-images --owners self
    ```

    Now that you have a Consul image, you're ready to provision Consul in a prod-like ASG.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Packer
    type: code
    hostname: cloud-client
    path: /root/packer
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 18000
- slug: provision-consul
  id: jhvf5b2rlhmk
  type: challenge
  title: 'Operations: Provision Consul'
  teaser: A short description of the challenge.
  assignment: |-
    In this assignment, you'll install the ASG that will run the Consul servers.
    The terraform module is configured to use the packer image you created in the last assignment. <br>

    Inspect the variable file, then deploy Consul.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform apply
    ```

    Now, validate the Consul servers deployed to AWS. <br>

    ```
    aws autoscaling describe-auto-scaling-groups
    ```

    You should see the Consul servers with a ``*-0.0.1` postfix.
    You will increment the deployment in the next exercise to finish the immutable bootstrap.

    Let's inspect one of the newly provisioned Consul instances.
    We can access the newly provisioned instance through our bastion host helper script.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    Go to the next assignment, and finish the bootstrap.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: bootstrap-consul
  id: 7mefmboypk7h
  type: challenge
  title: 'Operations: Bootstrap Consul'
  teaser: A short description of the challenge.
  assignment: |-
    Consul Enterprise supports an [upgrade pattern](https://learn.hashicorp.com/consul/day-2-operations/autopilot#upgrade-migrations) that allows operators to deploy a complete cluster of new servers,
    and safely migrate the new servers until the upgrade is complete. <br>

    In one of the terminal window, you will watch the new servers added as `non-voters`.
    Once autopilot has validated the new servers, it will safely promote them.
    You will see the old servers removed from the list.

    ```
    watch consul operator raft list-peers
    ```

    Next, validate the following two changes to the `consul.auto.tfvars file` to complete the bootstrap:
      * consul_cluster_version = "0.0.2"
      * bootstrap = false

    Now run terraform, and watch the servers automatically transition.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform apply
    ```

    Nice work! You just did an automated server update with Consul enterprise. <br>

    You can now validate the changes. Notice the `node_meta` is for the `0.0.2` version of the deployment. <br>

    ```
    consul catalog nodes --detailed
    ```

    You can also see that the `default_policy` for ACLs was updated for the new value.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    Now you can start setting up the tenants for the Consul shared service consumers.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: centralize-consul-secrets
  id: 8whoxkkzchx0
  type: challenge
  title: 'Security: Centralize Consul Secrets'
  teaser: A short description of the challenge.
  assignment: |-
    The assignment the participant needs to complete in order to proceed.

    You can use any GitHub flavoured markdown.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 300
- slug: create-namespaces-and-policies
  id: g7hylnrby0x9
  type: challenge
  title: 'Operations: Create Namespaces and Policies'
  teaser: A short description of the challenge.
  assignment: |-
    Consul enterprise support [namespaces](https://www.consul.io/docs/enterprise/namespaces/index.html) to enable network and operations teams to provide self-service for development teams.

    Start by creating the shared service discovery policy that will underpin the development team tenants.
    This will allow the mesh to discover services across teams, but teams will control the services that can connect to services they run. <br>

    ```
    consul acl policy create -name "cross-namespace-policy" -description "cross-namespace service discovery" -rules @cross-namespace.hcl
    ```

    Now you can create the namespaces for each development team, that share the namespace discovery policy. <br>

    ```
    consul namespace write frontend-namespace.hcl
    consul namespace write api-namespace.hcl
    ```

    You also need to create policies for the nodes that each team will run in their VPC.
    We will restrict this by CIDR block. <br>

    ```
    consul acl policy create -name "frontend-agent-policy" -description "frontend agents" -rules @frontend-team-agent.hcl
    consul acl policy create -name "api-agent-policy" -description "api agents" -rules @api-team-agent.hcl
    ```

    Nice work!
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Policies
    type: code
    hostname: cloud-client
    path: /root/policies
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8080
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: create-consul-roles
  id: r0oxqx8ait5u
  type: challenge
  title: 'Security: Create Consul Roles'
  teaser: A short description of the challenge.
  assignment: |-
    The assignment the participant needs to complete in order to proceed.

    You can use any GitHub flavoured markdown.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 300
- slug: create-vpc-peering
  id: suzx6ufcymhe
  type: challenge
  title: 'Network: Create Transitive Peering'
  teaser: A short description of the challenge.
  assignment: |-
    The network team is responsible for creating connectivity between VPCs, and on-prem environments.
    On AWS the network team leverages [AWS Transit Gateway](https://aws.amazon.com/transit-gateway/) to reduce management overhead. <br>

    Run Terraform to setup the transit gateway routes. <br>

    ```
    terraform apply -auto-approve
    ```

    Now inspect one of the route tables. Let's look at the route table for the API team private subnets.

    ```
    route_table=$(/usr/local/bin/terraform output -state /root/terraform/infra/terraform.tfstate -json api_private_route_table_ids | jq -r .[0])
    aws ec2 describe-route-tables --route-table-ids ${route_table}
    ```

    You'll notice routes to CIDR blocks for other application teams route through the transit gateway.  <br>

    You will expand the transit gateway to connect to the on-prem enviroment in a later assignment.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/network
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: provision-eks-clusters
  id: oxwep1bahgcl
  type: challenge
  title: 'Infrastructure: Provision EKS clusters'
  teaser: A short description of the challenge.
  assignment: |-
    The API and Frontend teams run their applications on AWS EKS.
    The infrastructure team is responsible for provisioning EKS clusters. <br>


    Inspect the Terraform for EKS, then provision the Kubernetes clusters.
    EKS can take over ~10 minutes to provision, so you will run this as a background task. <br>

    ```
    nohup terraform apply -auto-approve | tee terraform.out &
    ```

    Validate your clusters once they are provisioned.
    The status should be `ACTIVE`. <br>

    ```
    aws eks describe-cluster --name frontend
    aws eks describe-cluster --name api
    ```

    Great work! In this next assignments we will connect these Kubernetes nodes to the Consul shared service.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/eks
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 1800
- slug: configure-eks-cluster-segments
  id: xrckckfdqgj9
  type: challenge
  title: 'Operations: Configure EKS cluster segments'
  teaser: A short description of the challenge.
  assignment: |-
    To adhere to InfoSec policies, Consul gossip traffic needs to be restricted to App VPCs <> Shared Service VPC.
    Consul enterprise [network segments](https://www.consul.io/docs/enterprise/network-segments/index.html) support this hub-and-spoke topology.

    The network segment config has been added to the Terraform file with a new cluster version.
    Inspect it, then apply the configuration. <br>

    ```
    cat consul.auto.tfvars
    terraform apply -auto-approve
    ```

    Validate your network segments were applied to the Consul servers.

    ```
    check-consul-config /etc/consul.d/zz-override.json | jq
    ```

    Now you can configure these EKS nodes to Connect to the Consul service.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 1800
- slug: configure-eks-cluster-secrets
  id: ld9gbmuj5jnr
  type: challenge
  title: 'Operations: Configure EKS cluster secrets'
  teaser: A short description of the challenge.
  assignment: |-
    You need to seed the Consul clusters with secrets for the client agents to join with the server nodes.
    In production, a CI/CD pipeline, Terraform, or configuration management tool can perform these tasks with Vault.
    Platforms for these systems have well defined patterns for establishing trust with Vault. <br>

    You have been provided a sample script to the following:

    * Create a namespace to run Consul pods (injector webhooks, etc.)
    * Provide a gossip key
    * Provide an ACL

    Inspect the script.

    ```
    cat /usr/local/bin/setup-k8s-consul-cluster
    ```

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    ```

    Now configure the cluster. Add the secrets as arguments to the script

    ```
    setup-k8s-consul-cluster <gossip_key> <acl_token>
    ```

    Verify the secrets were provisioned.

    ```
    kubectl  get secrets -n consul
    ```

    Switch contexts, and repeat the process for the other EKS cluster.

    ```
    kubectl config use-context eks_api
    setup-k8s-consul-cluster <gossip_key> <acl_token>
    kubectl  get secrets -n consul
    ```

    Now you can provision the Consul client agents in the EKS clusters.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 1800
- slug: deploy-consul-in-eks
  id: 3ewjpnxsjsmh
  type: challenge
  title: 'Operations: Deploy Consul in EKS'
  teaser: A short description of the challenge.
  assignment: |-
    The Consul helm chart will allow us to easily deploy the agents into the cluster.
    The ACL init job will use the credentials you previously provisioned.

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    helm install hashicorp -f frontend-values.yaml --namespace consul ./consul-helm
    ```

    Next deploy the API clister.

    ```
    kubectl config use-context eks_api
    helm install hashicorp -f api-values.yaml --namespace consul ./consul-helm
    ```

    Nice work! Now you can start deploying applications.
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 300
checksum: "6141291402500543356"
